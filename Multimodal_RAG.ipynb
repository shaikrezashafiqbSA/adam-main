{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128.7 4273.8798\n",
      "4402.5797999999995\n"
     ]
    }
   ],
   "source": [
    "# 26/03/2024 Invoice\n",
    "b = 36.20 + 26.30 + 66.2\n",
    "s = 15000 * 0.28492532\n",
    "print(b, s)\n",
    "print(b+s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Librarian\n",
    "Librarian is a data-centric conversational model (RAG).\n",
    "\n",
    "It is able to embody archetypes (specialists) and is able to converse with the user to retrieve information from these databases.\n",
    "\n",
    "Such a data model centric design allows for the user to query information from a variety of databases, and the librarian is able to retrieve and cross-reference information from these databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.6.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached regex-2023.12.25-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Using cached tiktoken-0.6.0-cp312-cp312-win_amd64.whl (798 kB)\n",
      "Using cached regex-2023.12.25-cp312-cp312-win_amd64.whl (268 kB)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2023.12.25 tiktoken-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading specialist: TRAVELLER ...\n",
      "TRAVELLER embedding: TEST - CLIENT - LOADED\n",
      "TRAVELLER embedding: TEST - CLIENT REQUEST - LOADED\n",
      "TRAVELLER embedding: TEST - FLIGHTS - LOADED\n",
      "TRAVELLER embedding: TEST - ACCOMODATIONS - LOADED\n",
      "TRAVELLER embedding: TEST - ACTIVITIES - LOADED\n",
      "TRAVELLER embedding: TEST - SERVICES - LOADED\n",
      "TRAVELLER loaded\n"
     ]
    }
   ],
   "source": [
    "from RAG.librarian import Librarian \n",
    "\n",
    "librarian = Librarian(librarian_LLM_model = \"GEMINI\")\n",
    "\n",
    "# SELECT SPECIALIST DATABASE\n",
    "librarian.select_specialist(specialist = \"traveller\", specialist_LLM_model = \"GEMINI\")\n",
    "\n",
    "# Ask librarian to get acquinted with the specialist database\n",
    "librarian.Traveller.load_data_model(reembed = False,\n",
    "                                    embed_id = 0,\n",
    "                                    data_model_keys = {\"TEST - CLIENT\":\"CLIENT ID\",\n",
    "                                                        \"TEST - CLIENT REQUEST\":\"CLIENT ID\",\n",
    "                                                        \"TEST - FLIGHTS\":\"FLIGHT ID\",\n",
    "                                                        \"TEST - ACCOMODATIONS\":\"ACCOMODATION ID\",\n",
    "                                                        \"TEST - ACTIVITIES\":\"ACTIVITY ID\",\n",
    "                                                        \"TEST - SERVICES\":\"SERVICE ID\",\n",
    "                                                        },\n",
    "                                    reembed_table = {\"TEST - CLIENT\":True,\n",
    "                                                    \"TEST - CLIENT REQUEST\":True,\n",
    "                                                    \"TEST - FLIGHTS\":True,\n",
    "                                                    \"TEST - ACCOMODATIONS\":True,\n",
    "                                                    \"TEST - ACTIVITIES\":True,\n",
    "                                                    \"TEST - SERVICES\":True,\n",
    "                                                    }\n",
    "                                    )\n",
    "\n",
    "# Data Model data quality to be checked (dropped all irrelevant data - invoice number, other ids, empty cells)\n",
    "# TODO: disintegrate services into 3 buckets so that fit into a day or half  day, etc\n",
    "# eg; tour: 3h cultural tour at arab str, 4h foodie tour, 6h hiking tour, 3 hr shopping tour \n",
    "# - HOW TO CLASSIFY THESE BUCKETS? best way? gotta see existing travel packages (and how they are classified)\n",
    "# - have to check with SME on data model - edit, modify buckets based on SmartWorld specialised needs\n",
    "# prompt engineering: include a free-and easy day if theres a gap in the schedule or Reserve 1 day for RR, etc\n",
    "\n",
    "# RAG ACCURACY: \n",
    "# - how to check if the RAG is accurate? \n",
    "# - how to check if the RAG is hallucinating? \n",
    "# - how to check if the RAG is giving irrelevant data?\n",
    "\n",
    "#TODO: services -> activity (attractions, shopping, dining,) \n",
    "# \n",
    "# services new table\n",
    "# Services: drivers is daily price, if 7 day, 7 x daily price, tourguides, translator, chef, bodyguard, VIP escort\n",
    "\n",
    "\n",
    "#TODO: costing has to be accurate, and also has to be based on the client's budget\n",
    "\n",
    "#TODO: itinerary has to be based on the client's preferences, and also has to be based on the client's historical preferences\n",
    "\n",
    "#TODO: flawed or hallucinated responses have to be stored and set as bad example for future response \n",
    "# eg; this package is hallucinating dicsount vouchers at orchard road - this is a bad example, and should be flagged as such\n",
    "\n",
    "#TODO: streamline library (clear irrevelant backend codes; ADAM_llama_index_RAG, crypto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) _*Generate*_ : travel package from customer/agent prompt  + inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using initial query: Shaik from Kuala Lumpur wants to go Singapore for 5 days for 2 pax. Budget: $2000. Make one day specifically for Little India. Make leftover days for OTHER casual activities.\n",
      "Token size of the prompt for cl100k_base ~ 3372\n",
      "**Summary**\n",
      "\n",
      "Embark on an unforgettable journey to the vibrant city-state of Singapore, where you will immerse yourselves in a kaleidoscope of cultures and experience its captivating sights and flavors. Spend a day exploring the vibrant streets of Little India, where the air is filled with the scent of spices and the sounds of traditional music. Discover the iconic landmarks, such as the Merlion and the Botanic Gardens, on a guided tour. Delight in a delectable brunch in Kuala Lumpur, tantalizing your taste buds with local delicacies. Immerse yourselves in the historical and cultural tapestry of Chinatown and Kampong Glam, uncovering hidden gems and savoring authentic street food.\n",
      "\n",
      "**Journey Highlights**\n",
      "\n",
      "- Immerse yourself in the vibrant atmosphere of Little India\n",
      "- Marvel at the iconic Merlion and the lush Botanic Gardens\n",
      "- Tantalize your taste buds with a delectable local brunch in Kuala Lumpur\n",
      "- Discover the hidden gems and savor authentic street food in Chinatown and Kampong Glam\n",
      "\n",
      "**Itinerary**\n",
      "\n",
      "**Day 1**\n",
      "\n",
      "- 1600hrs: Fly to Singapore by flight (FLIGHT ID: 14)\n",
      "- 1800hrs: Check in at Raffles Hotel (ACCOMODATION ID: 7)\n",
      "- 2000hrs: Dinner at a traditional Indian restaurant in Little India (ACTIVITY ID: 17)\n",
      "\n",
      "**Day 2**\n",
      "\n",
      "- 0800hrs: Breakfast at Raffles Hotel (ACCOMODATION ID: 7)\n",
      "- 1000hrs: Explore the iconic Merlion and Botanic Gardens on a guided tour (ACTIVITY ID: 16)\n",
      "- 1400hrs: Lunch at a local restaurant in the Botanic Gardens\n",
      "- 1600hrs: Return to hotel\n",
      "\n",
      "**Day 3**\n",
      "\n",
      "- 0800hrs: Breakfast at Raffles Hotel (ACCOMODATION ID: 7)\n",
      "- 1000hrs: Fly to Kuala Lumpur by flight (FLIGHT ID: 8)\n",
      "- 1200hrs: Check in at Sheraton Hotel (ACCOMODATION ID: 12)\n",
      "- 1400hrs: Local Brunch Tour in Kuala Lumpur City (ACTIVITY ID: 15)\n",
      "\n",
      "**Day 4**\n",
      "\n",
      "- 0800hrs: Breakfast at Sheraton Hotel (ACCOMODATION ID: 12)\n",
      "- 1000hrs: Explore Chinatown on a guided tour (ACTIVITY ID: 18)\n",
      "- 1400hrs: Lunch at a local restaurant in Chinatown\n",
      "- 1600hrs: Return to hotel\n",
      "\n",
      "**Day 5**\n",
      "\n",
      "- 0800hrs: Breakfast at Sheraton Hotel (ACCOMODATION ID: 12)\n",
      "- 1000hrs: Explore Kampong Glam on a guided tour (ACTIVITY ID: 21)\n",
      "- 1400hrs: Lunch at a local restaurant in Kampong Glam\n",
      "- 1600hrs: Fly to Singapore by flight (FLIGHT ID: 9)\n",
      "\n",
      "**Highlights and Inclusions**\n",
      "\n",
      "- Accommodation at Raffles Hotel (ACCOMODATION ID: 7) in Singapore and Sheraton Hotel (ACCOMODATION ID: 12) in Kuala Lumpur\n",
      "- Flights from Kuala Lumpur to Singapore and back (FLIGHT ID: 8 and FLIGHT ID: 9)\n",
      "- Guided tours of Little India, the Merlion and Botanic Gardens, Chinatown, and Kampong Glam\n",
      "- Local Brunch Tour in Kuala Lumpur City\n",
      "- Travel insurance from Fairprice (SERVICE ID: 35)\n",
      "\n",
      "**Pricing**\n",
      "\n",
      "- Flights: $153 + $89 = $242\n",
      "- Accommodation: $200 x 2 nights + $2000 x 2 nights = $4400\n",
      "- Activities: $500 + $600 + $500 + $500 + $412 + $400 = $2912\n",
      "- Insurance: $50 + $40 = $90\n",
      "- **Total cost: $7644**\n"
     ]
    }
   ],
   "source": [
    "# Ask Traveller to generate a travel package\n",
    "default_query = \"Shaik from Kuala Lumpur wants to go Singapore for 5 days for 2 pax. Budget: $2000. Make one day specifically for Little India. Make leftover days for OTHER casual activities.\"\n",
    "\n",
    "initial_query = input(f\"Enter your initial query (or press Enter to use the default):\\n{default_query}\\n\")\n",
    "\n",
    "# Check if the user entered anything\n",
    "if not initial_query:\n",
    "  initial_query = default_query\n",
    "\n",
    "print(f\"Using initial query: {initial_query}\")\n",
    "\n",
    "\n",
    "convo_package = librarian.Traveller.III_generate_travel_package(initial_query = initial_query,\n",
    "                                                                 topN = 6, \n",
    "                                                                 model_name = \"gemini-pro\",\n",
    "                                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Regenerate: Follow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token size of the prompt for cl100k_base ~ 3557\n",
      "**Refined Travel Package**\n",
      "\n",
      "**Summary**\n",
      "\n",
      "Immerse yourself in the vibrant city-state of Singapore, where you will delve into a rich tapestry of cultures and savor its delectable flavors. Explore the bustling streets of Little India, discovering its aromatic spices and traditional music. Marvel at the iconic Merlion and the lush Botanic Gardens on a guided tour. Indulge in a delectable local brunch in Kuala Lumpur, tantalizing your taste buds with authentic Malaysian cuisine. Uncover the hidden gems and savor street food in Chinatown and Kampong Glam, experiencing the vibrant spirit of Singapore's diverse neighborhoods.\n",
      "\n",
      "**Journey Highlights**\n",
      "\n",
      "- Explore the vibrant atmosphere of Little India\n",
      "- Marvel at the iconic Merlion and the lush Botanic Gardens\n",
      "- Tantalize your taste buds with a delectable local brunch in Kuala Lumpur\n",
      "- Discover the hidden gems and savor street food in Chinatown and Kampong Glam\n",
      "\n",
      "**Itinerary**\n",
      "\n",
      "**Day 1**\n",
      "\n",
      "- 1600hrs: Fly to Singapore by flight (FLIGHT ID: 12)\n",
      "- 1800hrs: Check in at Mercure Bugis Hotel (ACCOMODATION ID: 11)\n",
      "- 2000hrs: Dinner at a traditional Indian restaurant in Little India (ACTIVITY ID: 17)\n",
      "\n",
      "**Day 2**\n",
      "\n",
      "- 0800hrs: Breakfast at Mercure Bugis Hotel (ACCOMODATION ID: 11)\n",
      "- 1000hrs: Explore the iconic Merlion and Botanic Gardens on a guided tour (ACTIVITY ID: 16)\n",
      "- 1400hrs: Lunch at a local restaurant in the Botanic Gardens\n",
      "- 1600hrs: Return to hotel\n",
      "\n",
      "**Day 3**\n",
      "\n",
      "- 0800hrs: Breakfast at Mercure Bugis Hotel (ACCOMODATION ID: 11)\n",
      "- 1000hrs: Fly to Kuala Lumpur by flight (FLIGHT ID: 8)\n",
      "- 1200hrs: Check in at Mandarin Hotel (ACCOMODATION ID: 9)\n",
      "- 1400hrs: Local Brunch Tour in Kuala Lumpur City (ACTIVITY ID: 15)\n",
      "\n",
      "**Day 4**\n",
      "\n",
      "- 0800hrs: Breakfast at Mandarin Hotel (ACCOMODATION ID: 9)\n",
      "- 1000hrs: Explore Chinatown on a guided tour (ACTIVITY ID: 18)\n",
      "- 1400hrs: Lunch at a local restaurant in Chinatown\n",
      "- 1600hrs: Return to hotel\n",
      "\n",
      "**Day 5**\n",
      "\n",
      "- 0800hrs: Breakfast at Mandarin Hotel (ACCOMODATION ID: 9)\n",
      "- 1000hrs: Explore Kampong Glam on a guided tour (ACTIVITY ID: 21)\n",
      "- 1400hrs: Lunch at a local restaurant in Kampong Glam\n",
      "- 1600hrs: Fly to Singapore by flight (FLIGHT ID: 9)\n",
      "\n",
      "**Highlights and Inclusions**\n",
      "\n",
      "- Accommodation at Mercure Bugis Hotel (ACCOMODATION ID: 11) in Singapore and Mandarin Hotel (ACCOMODATION ID: 9) in Kuala Lumpur\n",
      "- Flights from Kuala Lumpur to Singapore and back (FLIGHT ID: 8 and FLIGHT ID: 9)\n",
      "- Guided tours of Little India, the Merlion and Botanic Gardens, Chinatown, and Kampong Glam\n",
      "- Local Brunch Tour in Kuala Lumpur City\n",
      "- Travel insurance from Fairprice (SERVICE ID: 35)\n",
      "\n",
      "**Pricing**\n",
      "\n",
      "- Flights: $153 + $89 = $242\n",
      "- Accommodation: $200 x 2 nights + $1000 x 2 nights = $2400\n",
      "- Activities: $500 + $600 + $500 + $500 + $412 + $400 = $2912\n",
      "- Insurance: $50 + $40 = $90\n",
      "- **Total cost: $5644**\n"
     ]
    }
   ],
   "source": [
    "followup_query = input(\"Enter a followup query to refine the package (or press Enter to skip):\\n\")\n",
    "\n",
    "convo_package = librarian.Traveller.III_generate_travel_package(initial_query = \"\",\n",
    "                                                                followup_query = followup_query,\n",
    "                                                                 topN = 6, \n",
    "                                                                 model_name = \"gemini-pro\",\n",
    "                                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) OCR PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 5)) (0.4.0)\n",
      "Collecting google-cloud-aiplatform (from -r requirements.txt (line 6))\n",
      "  Using cached google_cloud_aiplatform-1.44.0-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Collecting anthropic (from -r requirements.txt (line 7))\n",
      "  Using cached anthropic-0.21.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting python-telegram-bot (from -r requirements.txt (line 10))\n",
      "  Downloading python_telegram_bot-21.0.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gspread (from -r requirements.txt (line 11))\n",
      "  Downloading gspread-6.0.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting PyPDF2 (from -r requirements.txt (line 12))\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shaik\\appdata\\roaming\\python\\python312\\site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openpyxl->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 5)) (2.28.1)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 5)) (2.17.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 5)) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 5)) (2.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 5)) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai->-r requirements.txt (line 5)) (4.10.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.4.0->google-generativeai->-r requirements.txt (line 5)) (1.23.0)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\shaik\\appdata\\roaming\\python\\python312\\site-packages (from google-cloud-aiplatform->-r requirements.txt (line 6)) (23.2)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform->-r requirements.txt (line 6))\n",
      "  Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform->-r requirements.txt (line 6))\n",
      "  Downloading google_cloud_bigquery-3.19.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform->-r requirements.txt (line 6))\n",
      "  Downloading google_cloud_resource_manager-1.12.3-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting shapely<3.0.0dev (from google-cloud-aiplatform->-r requirements.txt (line 6))\n",
      "  Downloading shapely-2.0.3-cp312-cp312-win_amd64.whl.metadata (7.2 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from anthropic->-r requirements.txt (line 7))\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic->-r requirements.txt (line 7))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from anthropic->-r requirements.txt (line 7))\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting sniffio (from anthropic->-r requirements.txt (line 7))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers>=0.13.0 (from anthropic->-r requirements.txt (line 7))\n",
      "  Using cached tokenizers-0.15.2-cp312-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting google-auth-oauthlib>=0.4.1 (from gspread->-r requirements.txt (line 11))\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting StrEnum==0.4.15 (from gspread->-r requirements.txt (line 11))\n",
      "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic->-r requirements.txt (line 7)) (3.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai->-r requirements.txt (line 5)) (1.62.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai->-r requirements.txt (line 5)) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai->-r requirements.txt (line 5)) (1.62.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai->-r requirements.txt (line 5)) (1.62.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 5)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 5)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 5)) (4.9)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib>=0.4.1->gspread->-r requirements.txt (line 11))\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.6.0 (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 6))\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0 (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 6))\n",
      "  Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->-r requirements.txt (line 6))\n",
      "  Downloading grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 6))\n",
      "  Downloading google-crc32c-1.5.0.tar.gz (12 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: certifi in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic->-r requirements.txt (line 7)) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic->-r requirements.txt (line 7))\n",
      "  Using cached httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->anthropic->-r requirements.txt (line 7))\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai->-r requirements.txt (line 5)) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shaik\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting huggingface_hub<1.0,>=0.16.4 (from tokenizers>=0.13.0->anthropic->-r requirements.txt (line 7))\n",
      "  Downloading huggingface_hub-0.22.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\shaik\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->google-generativeai->-r requirements.txt (line 5)) (0.4.6)\n",
      "Collecting filelock (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic->-r requirements.txt (line 7))\n",
      "  Downloading filelock-3.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic->-r requirements.txt (line 7))\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic->-r requirements.txt (line 7))\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shaik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->-r requirements.txt (line 5)) (2.2.1)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread->-r requirements.txt (line 11))\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading google_cloud_aiplatform-1.44.0-py2.py3-none-any.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 1.3/4.2 MB 27.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.5/4.2 MB 37.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.2 MB 37.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.2/4.2 MB 26.4 MB/s eta 0:00:00\n",
      "Downloading anthropic-0.21.3-py3-none-any.whl (851 kB)\n",
      "   ---------------------------------------- 0.0/851.6 kB ? eta -:--:--\n",
      "   --------------------------------------  849.9/851.6 kB 27.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 851.6/851.6 kB 10.8 MB/s eta 0:00:00\n",
      "Downloading python_telegram_bot-21.0.1-py3-none-any.whl (604 kB)\n",
      "   ---------------------------------------- 0.0/604.4 kB ? eta -:--:--\n",
      "   --------------------------------------  604.2/604.4 kB 37.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 604.4/604.4 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading gspread-6.0.2-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/53.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 53.9/53.9 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 232.6/232.6 kB 4.7 MB/s eta 0:00:00\n",
      "Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading google_cloud_bigquery-3.19.0-py2.py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   -------------------------------------- - 225.3/232.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 232.6/232.6 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading google_cloud_resource_manager-1.12.3-py2.py3-none-any.whl (333 kB)\n",
      "   ---------------------------------------- 0.0/333.7 kB ? eta -:--:--\n",
      "   --------------------------------------  327.7/333.7 kB 21.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 333.7/333.7 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n",
      "   ---------------------------------------- 0.0/125.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 125.6/125.6 kB 3.6 MB/s eta 0:00:00\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "Downloading shapely-2.0.3-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.4/1.4 MB 30.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 15.2 MB/s eta 0:00:00\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tokenizers-0.15.2-cp312-none-win_amd64.whl (2.2 MB)\n",
      "Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "   ---------------------------------------- 0.0/80.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 80.6/80.6 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl (25 kB)\n",
      "Downloading huggingface_hub-0.22.1-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 388.6/388.6 kB 12.2 MB/s eta 0:00:00\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
      "   -------------------------------------- - 163.8/172.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 172.0/172.0 kB 3.5 MB/s eta 0:00:00\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.7/151.7 kB 8.8 MB/s eta 0:00:00\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Downloading filelock-3.13.3-py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: google-crc32c\n",
      "  Building wheel for google-crc32c (pyproject.toml): started\n",
      "  Building wheel for google-crc32c (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for google-crc32c: filename=google_crc32c-1.5.0-py3-none-any.whl size=13046 sha256=9b180b7381536ec943f6aad0a8e23577c0dcc0fa75da81c8e5aa2f82f01f59fc\n",
      "  Stored in directory: c:\\users\\shaik\\appdata\\local\\pip\\cache\\wheels\\40\\b9\\2e\\089df51c5e6f7cda282c650457a1d9a7a55bf6386d8de8bf3d\n",
      "Successfully built google-crc32c\n",
      "Installing collected packages: StrEnum, sniffio, shapely, pyyaml, PyPDF2, oauthlib, h11, google-crc32c, fsspec, filelock, distro, requests-oauthlib, huggingface_hub, httpcore, google-resumable-media, anyio, tokenizers, httpx, grpc-google-iam-v1, google-auth-oauthlib, python-telegram-bot, gspread, google-cloud-core, anthropic, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "Successfully installed PyPDF2-3.0.1 StrEnum-0.4.15 anthropic-0.21.3 anyio-4.3.0 distro-1.9.0 filelock-3.13.3 fsspec-2024.3.1 google-auth-oauthlib-1.2.0 google-cloud-aiplatform-1.44.0 google-cloud-bigquery-3.19.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.12.3 google-cloud-storage-2.16.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 grpc-google-iam-v1-0.13.0 gspread-6.0.2 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 huggingface_hub-0.22.1 oauthlib-3.2.2 python-telegram-bot-21.0.1 pyyaml-6.0.1 requests-oauthlib-2.0.0 shapely-2.0.3 sniffio-1.3.1 tokenizers-0.15.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script distro.exe is installed in 'c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script httpx.exe is installed in 'c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tb-gcp-uploader.exe is installed in 'c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './database/quant/Shaik Reza Shafiq.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     37\u001b[0m pdf_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./database/quant/Shaik Reza Shafiq.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 38\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# table = create_table(text)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Print the table\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# for row in table:\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#     print(row)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m, in \u001b[0;36mextract_text_from_pdf\u001b[1;34m(pdf_file)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_pdf\u001b[39m(pdf_file):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Open the PDF file\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpdf_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# Create a PDF reader object\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         pdf_reader \u001b[38;5;241m=\u001b[39m PyPDF2\u001b[38;5;241m.\u001b[39mPdfReader(file)\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# Extract text from each page\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './database/quant/Shaik Reza Shafiq.pdf'"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    # Open the PDF file\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        # Create a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        # Extract text from each page\n",
    "        text = ''\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_table(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Initialize the table\n",
    "    table = []\n",
    "\n",
    "    # Iterate through the lines\n",
    "    for line in lines:\n",
    "        # Check if the line starts with a number\n",
    "        if re.match(r'^\\d+\\.', line):\n",
    "            # Split the line into columns\n",
    "            columns = re.split(r'\\s{2,}', line.strip())\n",
    "            table.append(columns)\n",
    "\n",
    "    return table\n",
    "\n",
    "# Example usage\n",
    "pdf_file = './database/quant/Shaik Reza Shafiq.pdf'\n",
    "text = extract_text_from_pdf(pdf_file)\n",
    "# table = create_table(text)\n",
    "\n",
    "# Print the table\n",
    "# for row in table:\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAIK REZA SHAFIQ  \n",
      "Shaik.reza.shafiq@gmail.com   | +65 9178 1248  \n",
      "linkedin.com/in/shaikrezashafiq | shaikrezashafiq@social  \n",
      "Nationality: Singapore Citizen  \n",
      "Age: 32 \n",
      "  \n",
      "EXPERIENCE   \n",
      " \n",
      " \n",
      "AI Data Engineering Consultant                     Feb 2024 - Present  \n",
      "For Smart World Travel Sdn Bhd (Consulting via Call Levels Pte Ltd) \n",
      "• Provided AI data engineering and consulting services to evaluate and prepare client companies for AI-\n",
      "related processing and automation.  \n",
      "• Conducted data audits, analyses and evaluations to assess readiness for AI adoption.  \n",
      "• Developed proof-of-concepts and prototypes for Retrieval Augmented Generation frameworks to \n",
      "leverage company’s proprietary data for value-add services (grounded large language models)  \n",
      "• Utilized expertise in prompt-engineering, fine-tuning and embeddings for large language models.  \n",
      "• Presented, delivered comprehensive reports and recommendations, for clients’ AI roadmap \n",
      " \n",
      " \n",
      "Call Levels Pte Ltd – Singapore                   Sep 2022 – Feb 2024 \n",
      "Quant Developer  \n",
      "• Back-office role, working closely with quant lead to develop, test and deploy factor models for hedging \n",
      "FX exposure and directional crypto trading. Leveraged boosting models, and shallow recurrent neural \n",
      "networks (RNNs). \n",
      "• Prototyped and developed applications using large language models for financial and non-financial uses \n",
      "such as fundamental analyses, investor personas, investor matching, food/recipe concierge. Adept at \n",
      "prompt-engineering, fine-tuning, embeddings. \n",
      " \n",
      "Alpha Stone Capital Pte Ltd – Singapore                                                         Sep 2021 – Aug 2022   \n",
      "Quant Developer (Crypto)  \n",
      "• Primarily a back-office role, working closely with traders to develop trading strategies, backtesting, \n",
      "building trading systems and conducting post-trade analyses.  \n",
      "• Developed an event-based backtesting library (Python) to test validity of technical analysis based \n",
      "hypotheses. Involves preprocessing, indicator calculation and postprocessing of 15million+ rows of \n",
      "historical price and volume data.   \n",
      "• Worked with traders to perform alpha research on trend-following, reversion and momentum strategies \n",
      "using price, volume liquidation, long\\short ratio, taker ratio and onchain data.  \n",
      "• Developed event-driven algorithmic trading system on an asynchronous framework  \n",
      "• Proficient in Python libraries such as Pandas, Plotly, Numpy, Numba, xgboost, catboost, sklearn, \n",
      "statsmodel  \n",
      "• Tech stack: Git, PostgreSQL, TimescaleDB, Docker and AWS and Linux PM2 for trading systems \n",
      "deployment.  \n",
      "  \n",
      "GIC Pte Ltd – Singapore                    Oct 2020 – Aug 2021 \n",
      "Quant Research Trainee (Equities)  \n",
      "• Primarily a back-office role, working closely with quants to assist in conducting alpha research and \n",
      "infrastructure development.  \n",
      "• Worked with head quant to research and develop smart betas based on forecast dispersion using inhouse \n",
      "backtesting library (R), utilizing AXIOMA risk factor and CAPIQ data.   \n",
      "• Developed parallelization feature for factor data calculation using R future package.  \n",
      "• Developed webscrapping tools for data library.   • Worked with head quant to improve on backtest analytics reporting, regarding sector and factor \n",
      "exposures.  \n",
      "• Worked with head quant to hold a Hackathon event to consolidate portfolio manager’s backtesting \n",
      "practices into a more standardized production grade signal research environment. Involves building \n",
      "repository and streamlining codebase for ease of backtesting and data processing.  \n",
      "• Proficient in Python libraries (Pandas, Numpy, Selenium, Dask), R libraries (Future, ggplot)  \n",
      "  \n",
      "CIMB-CGS Securities – Singapore                                                                        Sep 2018 - Jul 2019  \n",
      "Quant Research Intern (Equities)   \n",
      "• Primarily a front-office role working closely with traders to develop algorithmic strategies based on \n",
      "price and volume.  \n",
      "• Developed backtesting library (MATLAB) for prospective equities futures trading department.  \n",
      "• Performed alpha research on momentum and trend-following strategies.  \n",
      "• Competent in hyper-parameter and walk-forward optimization.  \n",
      "  \n",
      "EDUCATION   \n",
      " \n",
      "SINGAPORE MANAGEMENT UNIVERSITY                   Aug 2019 - Aug 2020  \n",
      "Master of Science in Quantitative  Finance   \n",
      "• Coursework covers options pricing and trading, portfolio management, risk management and machine \n",
      "learning.  \n",
      "• Covered stochastic differential equations for option pricing, designing algorithmic trading systems, \n",
      "profit and loss calculation and practicum of standard metrics (Sharpe, Sortino, etc)  \n",
      "• As part of a project, developed smart beta strategy using random forest regressor for portfolio weights, \n",
      "using standard style factor data, beating buy and hold.  \n",
      "• As part of a project, developed smart beta strategy using Lopez De Prado’s Hierarchical Risk Parity for \n",
      "constructing weights and position sizing using rolling retrained Gaussian mixture model on standard \n",
      "style factor data. Backtest concluded that strategy outperformed equal risk contribution portfolio.  \n",
      "• Competent in ML techniques such as ensemble/stacking of classifiers/ regressors and neural networks \n",
      "such as convolution and LSTM.  \n",
      "• Proficient in Python libraries such as Pandas, Numpy, matplotlib, sklearn, TensorFlow, Keras  \n",
      "  \n",
      "NANYANG TECHNOLOGICAL UNIVERSITY    Aug 2013 - May 2018  \n",
      "Bachelor of Science in Mathematics and Economics Certificate \n",
      "of Concentration in Computational Mathematics  \n",
      "• Coursework covers linear algebra, statistics, time series analysis, quantitative finance and \n",
      "macroeconomics and microeconomics.  \n",
      "• Hidden Markov Model and Stochastic/Deterministic Markov Decision Processes  \n",
      "• Other notable modules taken: Probabilistic Methods in Operations Research, Time Series Analysis, \n",
      "Numerical Analysis, Real Analysis, Optimization, Econometrics and Linear Algebra.  \n",
      "  \n",
      "ADDITIONAL   \n",
      " \n",
      "• Programming skills: Python, SQL, R, MATLAB, C++   \n",
      "• Interests: Play Tabla (the 2nd hardest musical instrument), rollerblading in the rain (dexterity and quick \n",
      "reflexes), singing, producing music videos covers and remixes on social media  \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test gdrive_handler\n",
    "from gdrive.gdrive_handler import GspreadHandler\n",
    "gsh = GspreadHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Dulman</td>\n",
       "      <td>32</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Sumbol</td>\n",
       "      <td>28</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob Yahya</td>\n",
       "      <td>45</td>\n",
       "      <td>Tokyo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name Age      City\n",
       "0  John Dulman  32  New York\n",
       "1  Jane Sumbol  28    London\n",
       "2    Bob Yahya  45     Tokyo"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsh.get_sheet_as_df(sheet_name = \"test sheet\", worksheet_name = \"Sheet1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 1), indices imply (1, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m caption_parts \u001b[38;5;241m=\u001b[39m image_caption\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m column_keys \u001b[38;5;241m=\u001b[39m [part\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m caption_parts]\n\u001b[1;32m---> 22\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PycharmProjects\\adam-main\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:856\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    848\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    849\u001b[0m             arrays,\n\u001b[0;32m    850\u001b[0m             columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    853\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    854\u001b[0m         )\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    866\u001b[0m         {},\n\u001b[0;32m    867\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    871\u001b[0m     )\n",
      "File \u001b[1;32md:\\PycharmProjects\\adam-main\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32md:\\PycharmProjects\\adam-main\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (1, 1), indices imply (1, 3)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = \"\"\"The image is a travel itinerary for the Kelantan Art & Culture Festival 2018. The festival is held from 23rd to 25th March 2018 at the Stadium Sultan Muhammad IV in Kelantan, Malaysia.\n",
    "\n",
    "The itinerary includes a list of events and activities that will be taking place during the festival. These include:\n",
    "\n",
    "- Cultural performances\n",
    "- Art exhibitions\n",
    "- Food stalls\n",
    "- Craft demonstrations\n",
    "- Music performances\n",
    "- Dance performances\n",
    "- Theater performances\n",
    "\n",
    "The itinerary also includes a map of the festival grounds and a list of participating artists and performers.\n",
    "\n",
    "The image is a valuable resource for anyone who is planning to attend the Kelantan Art & Culture Festival 2018. It provides a detailed overview of the events and activities that will be taking place, and it can help attendees to plan their trip.\n",
    "\"\"\"\n",
    "image_caption = \"supplier - flier - kelantan_2325mac2018\"\n",
    "caption_parts = image_caption.split('-')\n",
    "column_keys = [part.strip() for part in caption_parts]\n",
    "\n",
    "df = pd.DataFrame([data], columns=column_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaik\\AppData\\Local\\Temp\\ipykernel_9180\\864397749.py:12: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  sh.sheet1.update('A1', sample_data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '11GQ4pLXNffuEZH7r2d0LzBA4rjX7NviR7u-pbwjoFVY',\n",
       " 'updatedRange': 'Sheet1!A1:C4',\n",
       " 'updatedRows': 4,\n",
       " 'updatedColumns': 3,\n",
       " 'updatedCells': 12}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gspread\n",
    "sample_data = [\n",
    "    [\"Name\", \"Age\", \"City\"],\n",
    "    [\"John Dulman\", 32, \"New York\"],\n",
    "    [\"Jane Sumbol\", 28, \"London\"],\n",
    "    [\"Bob Yahya\", 45, \"Tokyo\"]\n",
    "    ]\n",
    "gc = gspread.service_account(filename='./gdrive/lunar-landing-389714-369d3f1b2a09.json')\n",
    "\n",
    "sh = gc.open(\"test sheet\")\n",
    "\n",
    "sh.sheet1.update('A1', sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pillow (from pdf2image)\n",
      "  Downloading pillow-10.2.0-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from pytesseract) (23.2)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Downloading pillow-10.2.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.6/2.6 MB 55.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 55.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pillow, pytesseract, pdf2image\n",
      "Successfully installed pdf2image-1.17.0 pillow-10.2.0 pytesseract-0.3.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdf2image pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdf2image\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    # Open the PDF file\n",
    "    images = pdf2image.convert_from_path(pdf_file)\n",
    "\n",
    "    # Extract text from each page\n",
    "    text = ''\n",
    "    for image in images:\n",
    "        # Convert the image to a byte stream\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        image.save(img_byte_arr, format='PNG')\n",
    "        img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "        # Extract text from the image\n",
    "        text += pytesseract.image_to_string(Image.open(io.BytesIO(img_byte_arr)), lang='ara+eng')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_file = 'your_pdf_file.pdf'\n",
    "text = extract_text_from_pdf(pdf_file)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accommodation Requirements September 202+ Period of stay Destination Bali/Jakarta/Bandung FIT (less than ' OR Group (above 10 pax) Type of Client Malaysian Nationality 40-50 years old Agc Efoup Number of pax Adult (12 years old and above) Age Children (11 years old below} 6/8/11 3/4/5 Hatel rating Kindly chaose ane &s Per below options: Property Hotel VApartment/villa/ Resort Room Units requircd 15 units of twin snaring Standard room Room catecony Preferably two sinEle beds Bedding type Kindly quote rojm rate with daily breakiast for daily mnner lunction room per Budget Please quolc separate rate pcason Mezl Falf B/D OR Full Board: WLD Pufdose Fisutt Transportation Requirements Type : vchicle Airport > Hotel > Anpont Transfers Khndly advise vou wculd require for return airport transfers Only Usage Kindi Aoakeilvov Wouic require for daily usage of 08 nouis per Qay Tout 'ackage Requirements Tour Places Kindly Jc.ise You have any preferred iocations visit Type Activities Kindly advise  family Iriendi; OR adventurous activilles Kindly select as per below options: City Tour Cultural Tour Tour preferences 4qventure Beach Resort Pack JEe Wildlife Nature Touts Luxury Tour Meal Halal arrangement / No Pork Na Lard Flight Requirements Seat Category Econamy Class OR Dusiness Class OR Mix MAS Preferred Alrline options Malirido Remark recommend forall of our clients to fly only via KLIA Terminal ! Ipa ) 40/50 Typc Duttet Board, itwi wauld\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "def extract_text_from_image(image_file):\n",
    "    # Create a reader for the languages you want to support\n",
    "    reader = easyocr.Reader(['en'])\n",
    "\n",
    "    # Use the reader to read the text from the image\n",
    "    result = reader.readtext(image_file)\n",
    "\n",
    "    # The result is a list of tuples, where each tuple represents a block of text.\n",
    "    # The first element of the tuple is the coordinates of the block, and the second element is the text.\n",
    "    # You can extract just the text like this:\n",
    "    text = ' '.join([block[1] for block in result])\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "image_file = './database/travel/Ingest/Cust_req_1.jpg'\n",
    "text = extract_text_from_image(image_file)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Accommodation Requirements September 202+ Period of stay Destination Bali/Jakarta/Bandung FIT (less than ' OR Group (above 10 pax) Type of Client Malaysian Nationality 40-50 years old Agc Efoup Number of pax Adult (12 years old and above) Age Children (11 years old below} 6/8/11 3/4/5 Hatel rating Kindly chaose ane &s Per below options: Property Hotel VApartment/villa/ Resort Room Units requircd 15 units of twin snaring Standard room Room catecony Preferably two sinEle beds Bedding type Kindly quote rojm rate with daily breakiast for daily mnner lunction room per Budget Please quolc separate rate pcason Mezl Falf B/D OR Full Board: WLD Pufdose Fisutt Transportation Requirements Type : vchicle Airport > Hotel > Anpont Transfers Khndly advise vou wculd require for return airport transfers Only Usage Kindi Aoakeilvov Wouic require for daily usage of 08 nouis per Qay Tout 'ackage Requirements Tour Places Kindly Jc.ise You have any preferred iocations visit Type Activities Kindly advise  family Iriendi; OR adventurous activilles Kindly select as per below options: City Tour Cultural Tour Tour preferences 4qventure Beach Resort Pack JEe Wildlife Nature Touts Luxury Tour Meal Halal arrangement / No Pork Na Lard Flight Requirements Seat Category Econamy Class OR Dusiness Class OR Mix MAS Preferred Alrline options Malirido Remark recommend forall of our clients to fly only via KLIA Terminal ! Ipa ) 40/50 Typc Duttet Board, itwi wauld\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.44.0-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.17.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-cloud-aiplatform) (2.28.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-cloud-aiplatform) (4.25.3)\n",
      "Requirement already satisfied: packaging>=14.3 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-cloud-aiplatform) (23.2)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform)\n",
      "  Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform)\n",
      "  Downloading google_cloud_bigquery-3.19.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
      "  Downloading google_cloud_resource_manager-1.12.3-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: shapely<3.0.0dev in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-cloud-aiplatform) (2.0.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.6.0 (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0 (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
      "  Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform)\n",
      "  Downloading grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
      "  Downloading google-crc32c-1.5.0.tar.gz (12 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy<2,>=1.14 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.5.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.2.2)\n",
      "Downloading google_cloud_aiplatform-1.44.0-py2.py3-none-any.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 3.5/4.2 MB 73.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.2/4.2 MB 67.0 MB/s eta 0:00:00\n",
      "Downloading google_cloud_bigquery-3.19.0-py2.py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 232.6/232.6 kB ? eta 0:00:00\n",
      "Downloading google_cloud_resource_manager-1.12.3-py2.py3-none-any.whl (333 kB)\n",
      "   ---------------------------------------- 0.0/333.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 333.7/333.7 kB 20.2 MB/s eta 0:00:00\n",
      "Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n",
      "   ---------------------------------------- 0.0/125.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 125.6/125.6 kB 7.2 MB/s eta 0:00:00\n",
      "Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "   ---------------------------------------- 0.0/80.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 80.6/80.6 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl (25 kB)\n",
      "Building wheels for collected packages: google-crc32c\n",
      "  Building wheel for google-crc32c (pyproject.toml): started\n",
      "  Building wheel for google-crc32c (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for google-crc32c: filename=google_crc32c-1.5.0-py3-none-any.whl size=13046 sha256=f8f2df662fe3e77d87b577c8cecd10d6ab9631e14d738c62af16f2b29bc9382f\n",
      "  Stored in directory: c:\\users\\shaik\\appdata\\local\\pip\\cache\\wheels\\40\\b9\\2e\\089df51c5e6f7cda282c650457a1d9a7a55bf6386d8de8bf3d\n",
      "Successfully built google-crc32c\n",
      "Installing collected packages: google-crc32c, google-resumable-media, grpc-google-iam-v1, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "Successfully installed google-cloud-aiplatform-1.44.0 google-cloud-bigquery-3.19.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.12.3 google-cloud-storage-2.16.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 grpc-google-iam-v1-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerative_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerativeModel\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m GenerativeModel(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.0-pro-vision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is this?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mimg\u001b[49m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "from vertexai import generative_models\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "model = GenerativeModel(model_name=\"gemini-1.0-pro-vision\")\n",
    "\n",
    "response = model.generate_content([\"What is this?\", img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Librarian's Traveller Other CAPABILITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) _Retrieval_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Passage': 'CLIENT ID: C139, Client Name: DATO SRI SHAIK AQMAL BIN SHAIK ALAUDDIN, Description: TRAVEL INSURANCE & ACOMMODATION 11 - 18 FEB 2024, Date: 08 Feb 24, Price: 22000.0, Remarks: nan, PREPARED BY: ANIES',\n",
       "  'Similarity Score': 0.6459325350732584},\n",
       " {'Passage': 'CLIENT ID: C133, Client Name: AHMAD SABRI BIN MOHD TAHIR, Description: ACCOMMODATION , Date: 2024-03-01 00:00:00, Price: 800.0, Remarks: nan, PREPARED BY: ANIES',\n",
       "  'Similarity Score': 0.6349598078097383},\n",
       " {'Passage': 'CLIENT ID: C139, Client Name: DATO SRI SHAIK AQMAL BIN SHAIK ALAUDDIN, Description: AIR TICKET KUL-MED-JED-IST-MUC-IST-KUL & VISA ARRANGEMENT, Date: 2024-02-06 00:00:00, Price: 42280.0, Remarks: UMRAH & ZIARAH, PREPARED BY: SYAZANA',\n",
       "  'Similarity Score': 0.6323799378574627}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"shaik aqmal\"\n",
    "client_recommendations = librarian.Traveller.I_recommend_client(content = prompt,\n",
    "                                                        topN=3,\n",
    "                                                        task_type = \"retrieval_document\",\n",
    "                                                        # task_type = \"retrieval_query\",\n",
    "                                                        )\n",
    "client_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Passage': 'CLIENT ID: C139, Date: 2014-03-24 00:00:00, Location: kuala lumpur, Prompt: dunno, just plan something exciting and cultural with calm vibe around an important AI event of mine on december 24. So i need to be mentally sharp before, and not shagged, Duration: 1 month, Period: 2014-12-12 to 2015-01-12, Budget: 40000, Pax: 2, Client Quirks: doesnt like humid weather and will always prefer somewhere cold, Special Requests: Needs 2 bodyguards at all times',\n",
       "  'Similarity Score': 0.6404644571132381}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Who is the most hardcore client?\"\n",
    "client_recommendations = librarian.Traveller.I_recommend_client_request(content = prompt,\n",
    "                                                        topN=1,\n",
    "                                                        # task_type = \"retrieval_document\",\n",
    "                                                        task_type = \"retrieval_query\",\n",
    "                                                        ) \n",
    "client_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries (assuming you have necessary libraries installed)\n",
    "import numpy as np\n",
    "\n",
    "# Define weights (you can adjust these based on your needs)\n",
    "PROMPT_WEIGHT = 0.8  # Higher weight for user's current interest\n",
    "PROFILE_WEIGHT = 0.2  # Lower weight for user profile\n",
    "\n",
    "def weighted_search(prompt_embedding, user_profile_embedding, recommendations):\n",
    "  \"\"\"\n",
    "  Combines prompt and user profile embedding similarity scores for weighted search.\n",
    "\n",
    "  Args:\n",
    "      prompt_embedding: Embedding representing the user's prompt.\n",
    "      user_profile_embedding: Embedding representing the user's travel profile.\n",
    "      recommendations: List of recommendations (flights, accommodations, services).\n",
    "\n",
    "  Returns:\n",
    "      A list of recommendations sorted by their weighted similarity score.\n",
    "  \"\"\"\n",
    "  # Calculate similarity scores between prompt embedding and each recommendation\n",
    "  prompt_similarities = [np.dot(prompt_embedding, item['embedding']) for item in recommendations]\n",
    "\n",
    "  # Calculate similarity scores between user profile embedding and each recommendation\n",
    "  profile_similarities = [np.dot(user_profile_embedding, item['embedding']) for item in recommendations]\n",
    "\n",
    "  # Combine similarity scores with weights\n",
    "  weighted_scores = [PROMPT_WEIGHT * prompt_sim + PROFILE_WEIGHT * profile_sim \n",
    "                     for prompt_sim, profile_sim in zip(prompt_similarities, profile_similarities)]\n",
    "\n",
    "  # Sort recommendations based on weighted scores (descending order)\n",
    "  sorted_recommendations = sorted(zip(recommendations, weighted_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  return [item[0] for item in sorted_recommendations]\n",
    "\n",
    "# Example usage (assuming you have functions to generate prompt and user profile embeddings, \n",
    "# and a list of recommendations with embedding information)\n",
    "prompt_embedding = generate_prompt_embedding(\"I want to go to Singapore\")\n",
    "user_profile_embedding = generate_user_profile_embedding(user_id)\n",
    "recommendations = get_recommendations(\"Singapore\")  # Flights, accommodations, services for Singapore\n",
    "\n",
    "personalized_recommendations = weighted_search(prompt_embedding, user_profile_embedding, recommendations)\n",
    "\n",
    "# Use the personalized_recommendations list for further processing or display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) _Recommend_ : travel logistics (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Passage': 'FLIGHT ID: 12, Origin: Singapore, Destination: Kuala Lumpur, Price: from $82 ',\n",
       "  'Similarity Score': 0.7052937218642239},\n",
       " {'Passage': 'FLIGHT ID: 14, Origin: Singapore, Destination: Kuala Lumpur, Price: from $70 ',\n",
       "  'Similarity Score': 0.7012338995802883},\n",
       " {'Passage': 'FLIGHT ID: 13, Origin: Singapore, Destination: Kuala Lumpur, Price: from $153 ',\n",
       "  'Similarity Score': 0.7012112802307542},\n",
       " {'Passage': 'FLIGHT ID: 7, Origin: Kuala Lumpur, Destination: Singapore, Price: from $83',\n",
       "  'Similarity Score': 0.6982578586274387},\n",
       " {'Passage': 'FLIGHT ID: 8, Origin: Kuala Lumpur, Destination: Singapore, Price: from $101',\n",
       "  'Similarity Score': 0.6953344564516624}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = f\"What flights from Singapore to Kuala Lumpur?\"\n",
    "\n",
    "flight_recommendations = librarian.Traveller.I_recommend_flights(content,\n",
    "                                                                  topN = 5, \n",
    "                                                                #  task_type=\"retrieval_document\")\n",
    "                                                                 task_type = \"retrieval_query\")\n",
    "flight_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Passage': 'FLIGHT ID: 12, Origin: Singapore, Destination: Kuala Lumpur, Price: from $82 ',\n",
       "  'Similarity Score': 0.7622100129085936},\n",
       " {'Passage': 'FLIGHT ID: 8, Origin: Kuala Lumpur, Destination: Singapore, Price: from $101',\n",
       "  'Similarity Score': 0.7621598208444731},\n",
       " {'Passage': 'FLIGHT ID: 9, Origin: Kuala Lumpur, Destination: Singapore, Price: from $89',\n",
       "  'Similarity Score': 0.7549945336128664},\n",
       " {'Passage': 'FLIGHT ID: 10, Origin: Kuala Lumpur, Destination: Michigan, Singapore, Price: from $1200',\n",
       "  'Similarity Score': 0.7543854603831508},\n",
       " {'Passage': 'FLIGHT ID: 7, Origin: Kuala Lumpur, Destination: Singapore, Price: from $83',\n",
       "  'Similarity Score': 0.7534204782143469},\n",
       " {'Passage': 'FLIGHT ID: 14, Origin: Singapore, Destination: Kuala Lumpur, Price: from $70 ',\n",
       "  'Similarity Score': 0.7520427625441319},\n",
       " {'Passage': 'FLIGHT ID: 13, Origin: Singapore, Destination: Kuala Lumpur, Price: from $153 ',\n",
       "  'Similarity Score': 0.7500689235748557},\n",
       " {'Passage': 'FLIGHT ID: 23, Origin: Michigan, Singapore, Destination: Kuala Lumpur, Price: from $1200',\n",
       "  'Similarity Score': 0.7464974369278223},\n",
       " {'Passage': 'FLIGHT ID: 17, Origin: Denpasar Bali, Destination: Kuala Lumpur, Price: from $193',\n",
       "  'Similarity Score': 0.7370804883500821},\n",
       " {'Passage': 'FLIGHT ID: 16, Origin: Denpasar Bali, Destination: Kuala Lumpur, Price: from $252',\n",
       "  'Similarity Score': 0.7357880128858286}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = f\"origin Kuala Lumpur, destination Singapore\"\n",
    "\n",
    "flight_recommendations = librarian.Traveller.I_recommend_flights(content,\n",
    "                                                                 topN = 10, \n",
    "                                                                 task_type=\"retrieval_query\")\n",
    "                                                                #  task_type = \"semantic_similarity\")\n",
    "flight_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Client: Shaik\\nClient Request: The client Shaik needs to go Singapore with VIP driver for 6 days, with a focus on chinatown for 1 day. then everything else is free and easy\\nFlights: Singapore airport\\nAccommodations: Singapore\\nActivities: Chinatown tour\\nServices: VIP driver for 6 days'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_query = \"The client Shaik needs to go Singapore with VIP driver for 6 days, with a focus on chinatown for 1 day. then everything else is free and easy\"\n",
    "prompt = f\"\"\"You are a prompt engineer for a travel company. Given the following unstructured request from a client: {initial_query}\n",
    "                        Segment the query into:\n",
    "                        1) Client\n",
    "                        2) Client Request\n",
    "                        3) Flights\n",
    "                        4) Accomodations\n",
    "                        5) Activities\n",
    "                        6) Services, \n",
    "                        So for example, if a client request is \"The client Shaik needs to go Singapore with VIP driver for 5 days, with a focus on arab/malay street\",\n",
    "                        you would segment it into and return:\n",
    "                        'client: Shaik\n",
    "                        client_request: The client Shaik needs to go Singapore with VIP driver for 5 days, with a focus on arab/malay street\n",
    "                        flights: Singapore airport\n",
    "                        accomodations: near arab/malay street, Singapore\n",
    "                        activities: segmented_query:arab/malay street tour\n",
    "                        services: segmented_query:VIP driver for 5 days'\n",
    "                        ALL segments must be returned.\n",
    "                        If any of the segments are not present, then please indicate that the segment is not present and provide fillers. \n",
    "                        \"\"\"\n",
    "response = librarian.Traveller.model_specialist.prompt(prompt)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Client: Shaik',\n",
       " 'Client Request: The client Shaik needs to go Singapore with VIP driver for 6 days, with a focus on chinatown for 1 day. then everything else is free and easy',\n",
       " 'Flights: Singapore airport',\n",
       " 'Accommodations: Singapore',\n",
       " 'Activities: Chinatown tour',\n",
       " 'Services: VIP driver for 6 days']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the segments\n",
    "segments = response.text.split(\"\\n\")\n",
    "\n",
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('flights:\\n'\n",
      " \"[{'Passage': 'FLIGHT ID: 10, Origin: Kuala Lumpur, Destination: Michigan, \"\n",
      " \"Singapore, Price: from $1200', 'Similarity Score': 0.5314661234074356}, \"\n",
      " \"{'Passage': 'FLIGHT ID: 7, Origin: Kuala Lumpur, Destination: Singapore, \"\n",
      " \"Price: from $83', 'Similarity Score': 0.5302942126398107}, {'Passage': \"\n",
      " \"'FLIGHT ID: 15, Origin: Singapore, Destination: Michigan, Singapore, Price: \"\n",
      " \"from $1300', 'Similarity Score': 0.5281868848282231}]\")\n",
      "('accomodations:\\n'\n",
      " \"[{'Passage': 'ACCOMODATION ID: 7, Name: Raffles hotel, Type: Hotel, \"\n",
      " 'Location: Singapore, Price per night: 200, Description: An iconic **5-star '\n",
      " 'luxury beachfront resort**, exudes timeless elegance. After an extensive '\n",
      " 'restoration, it retains its legendary charm, service, and heritage. Explore '\n",
      " 'newly opened bars, restaurants, and courtyards. The **Long Bar**, home of '\n",
      " 'the famous Singapore Sling, returns fully restored. Located in the heart of '\n",
      " 'the city, close to financial districts, cultural sights, and shopping, '\n",
      " 'Raffles Hotel is a gracious landmark where history meets impeccable service '\n",
      " \"for over 125 years 🌴🌟.\\\\n', 'Similarity Score': 0.6017507898776795}, \"\n",
      " \"{'Passage': 'ACCOMODATION ID: 9, Name: Mandarin Oriental, Type: Hotel, \"\n",
      " 'Location: Singapore, Price per night: 1000, Description: 5 star hotel '\n",
      " \"services, with 5 star breakfast and dinner but no oyster ', 'Similarity \"\n",
      " \"Score': 0.5946050234210385}, {'Passage': 'ACCOMODATION ID: 15, Name: Ghost \"\n",
      " 'Hotel , Type: Colonial Guesthouse, Location: Singapore, Michigan, Price per '\n",
      " 'night: 9090, Description: A stay for not for the faint of heart as the '\n",
      " \"butlers and hotel staff are all in 1800s getup and theme', 'Similarity \"\n",
      " \"Score': 0.5913199062488144}]\")\n",
      "('activities:\\n'\n",
      " \"[{'Passage': 'ACTIVITY ID: 19, Activity: Arab Street cultural foodie tours, \"\n",
      " 'Location: Singapore, Price: $500 per 4 pax, Duration: 5 hours, Category: '\n",
      " \"Tour, Dining, Shopping', 'Similarity Score': 0.682075094609117}, {'Passage': \"\n",
      " \"'ACTIVITY ID: 17, Activity: Little India cultural foodie tours, Location: \"\n",
      " 'Singapore, Price: $500 per 4 pax, Duration: 5 hours, Category: Tour, Dining, '\n",
      " \"Shopping', 'Similarity Score': 0.6323447178364427}, {'Passage': 'ACTIVITY \"\n",
      " 'ID: 18, Activity: ChinaTown cultural foodie tours, Location: Singapore, '\n",
      " \"Price: $500 per 4 pax, Duration: 5 hours, Category: Tour, Dining, Shopping', \"\n",
      " \"'Similarity Score': 0.6101327941170755}]\")\n",
      "('services:\\n'\n",
      " \"[{'Passage': 'SERVICE ID: 30, Services: Sedan driver (Mercedes S Class, 3 \"\n",
      " 'passengers, VIP), Location: Singapore, Price: $800 per 4 pax per day, '\n",
      " \"Category: Driver', 'Similarity Score': 0.6747843775052251}, {'Passage': \"\n",
      " \"'SERVICE ID: 31, Services: MPV driver (Toyota Alphard, 8 passengers, VIP), \"\n",
      " \"Location: Singapore, Price: $1200 per 8 pax per day, Category: Driver', \"\n",
      " \"'Similarity Score': 0.65190925804893}, {'Passage': 'SERVICE ID: 28, \"\n",
      " 'Services: Tourguide + Translator , Location: Singapore, Price: $100 per '\n",
      " \"group per hour, Category: Translator, Guide', 'Similarity Score': \"\n",
      " '0.650376114737962}]')\n"
     ]
    }
   ],
   "source": [
    "# Ask librarian a question regarding one of the specialist databases WITH a chatbot wrapper\n",
    "# input by agent \n",
    "# FUTURE: email thread, entire convo packages - given relevant buckets, eg; dest, duration , budget\n",
    "\n",
    "# TODO LATER: need relevance score to cut out irrelevant data\n",
    "travel_proposal = \"i need to go Singapore with VIP driver for 5 days, with a focus on arab/malay street\"\n",
    "convo_package = librarian.Traveller.II_recommend_travel_logistics(travel_proposal = travel_proposal,\n",
    "                                                                   topN=3,\n",
    "                                                                   chatbot = False, \n",
    "                                                                   chatbot_model_name = \"gemini-pro\")\n",
    "\n",
    "# Highlight: these outputs are from YOUR preferred buckets (your own inventory) but using gemini/openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) _*Generate*_ : travel proposal (itinerary) from customer request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The image is of the Sultan Mosque in Singapore. It is a beautiful mosque '\n",
      " 'located in the Kampong Glam district. The mosque is open to visitors from '\n",
      " 'all faiths and is a popular tourist destination.\\n'\n",
      " '\\n'\n",
      " 'Day 1:\\n'\n",
      " '\\n'\n",
      " '* Arrive in Singapore and check into your hotel.\\n'\n",
      " '* Take a walk around the Kampong Glam district and visit the Sultan Mosque.\\n'\n",
      " '* Have dinner at a traditional Malay restaurant.\\n'\n",
      " '\\n'\n",
      " 'Day 2:\\n'\n",
      " '\\n'\n",
      " '* Visit the Gardens by the Bay.\\n'\n",
      " '* Take a boat ride on the Singapore River.\\n'\n",
      " '* Have dinner at a seafood restaurant.\\n'\n",
      " '\\n'\n",
      " 'Day 3:\\n'\n",
      " '\\n'\n",
      " '* Visit the Universal Studios Singapore theme park.\\n'\n",
      " '* Go shopping at the Orchard Road shopping district.\\n'\n",
      " '* Have dinner at a rooftop restaurant with views of the city.\\n'\n",
      " '\\n'\n",
      " 'Free and easy day:\\n'\n",
      " '\\n'\n",
      " '* Visit the Singapore Zoo.\\n'\n",
      " '* Go to the Singapore Botanic Gardens.\\n'\n",
      " '* Take a walk through the Chinatown district.\\n'\n",
      " '* Have dinner at a Peranakan restaurant.')\n"
     ]
    }
   ],
   "source": [
    "# Ask librarian a question regarding one of the specialist databases\n",
    "travel_proposal_request = \"i need to go Singapore for a week, like in this image. Please plan 3 day trip for this country (with 1 day specifically for the location in the image), then a free and easy day where you can fill it up with as much activities as possible from the \"\n",
    "convo_package = librarian.Traveller.II_generate_travel_proposal(input_prompt = travel_proposal_request,\n",
    "                                                                model_name = \"gemini-pro\",\n",
    "                                                                image_path = \"./database/travel/sometiktokss.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: flawed or hallucinated responses have to be stored and set as bad example for future response \n",
    "# eg; this package is hallucinating dicsount vouchers at orchard road - this is a bad example, and should be flagged as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SELECT SPECIALIST DATABASE\n",
    "# librarian.select_specialist(specialist = \"muslim\", specialist_LLM_model = \"GEMINI\")\n",
    "# librarian.Muslim.load_data_model(reembed = False,\n",
    "#                                     data_model_keys = {\"Quran\":\"SURAH ID\",\n",
    "#                                                         \"Hadith\":\"HADITH ID\",\n",
    "#                                                         \"Tafsir\":\"TAFSIR ID\",\n",
    "#                                                         }\n",
    "#                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Notes:\n",
    "at 35:34\n",
    "text Splitting has to be done by relevance ->embeddings_search --> topN -> chain back to LLM --> talk to AI (+relevance)\n",
    "\n",
    "at 41:30\n",
    "Okay nice, he is explanining RAG nicely :\n",
    "my thoughts at that timestamp:\n",
    "TECH LEVELS:\n",
    "1) image + text  -> text \n",
    "    - No context: LLM only\n",
    "    - eg; LLM(\"Hey AI, give me information on X\")\n",
    "2) image + text -> text  \n",
    "    - Basic Contextualist: Context sample + LLM\n",
    "    - eg; LLM(\"Hey AI, give me information on X, given (Context sample)\")\n",
    "3) image + text -> text \n",
    "    - Big Contextualist: Context Population + Recommendation engine + LLM\n",
    "    - eg; RAG(\"Hey AI, give me information on X\", Librarian(topN, X, Context Population))\n",
    "\n",
    "at44:31, yes nice explanation of dimensionality reduction\n",
    "\n",
    "\n",
    "at 46:55: have to classify all avialable LLMs by\n",
    "matching capabilities, (based on Baijun's picture for example, CLAUDE has best reasoning --> does this imply better model embeddings?)\n",
    "COST (token, contextualising cost)\n",
    "\n",
    "\n",
    "at 51:38:\n",
    "macthing parameters (similiarity)\n",
    "cosine most popular, but could there be other techiques could have better edge at certain data?? (to investigate)\n",
    "\n",
    "\n",
    "\n",
    "at 53:50:\n",
    "VECTOR STORE\n",
    "db for embeddings/vectors\n",
    "to store all documents, auto gen emebddings, store auto\n",
    "!!! Vector stores are optimised to do similiarity really WELL\n",
    "sure anot, what optimisation, should we built everything inhouse to not be dependant. all this just funnels to get to pay for pinecone only. but yea gotta check cost-benefit here\n",
    "i've already make everything up till 56mins from scratch already, we already have.\n",
    "just maybe vector stores. must compare pricing and efficacy of available vector stores\n",
    "\n",
    "\n",
    "at 59:04:\n",
    "VECTOR STORE features\n",
    "vector_store.as_retriever\n",
    "connects directly to vector_store to get topN\n",
    "langchain is a wrapper for LLMs. but gemini already has this feature built in.\n",
    "scope: we aggregate all this LLM services. but remain portable and Microservices oriented as any one of them cld be bought over by corps (no new updates), etc\n",
    "\n",
    "61:09 onwards is dry pinecone stuff\n",
    "but yea could have features must replicate/generalise\n",
    "\n",
    "at 1:01:33\n",
    "LangChain codebase/structure highlights\n",
    "prompts themselves are CLASSES, which are modularly attached to RETRIEVER_CLASS\n",
    "whereas mine is functions only. Need persistence!!\n",
    "\n",
    "at1:04:20:\n",
    "langchain codebase highlights\n",
    "vector_stores are wrapped in highly functionable classes\n",
    "whereas mine is all objects\n",
    "\n",
    "\n",
    "at 1:06:46:\n",
    "Langchain UX experience and ease of \"chaining\":\n",
    "(context + prompt + model + parser ) + prompt\n",
    "so its an infinite way to arrange this.\n",
    "this is basically modelling conversations with a librarian, and then to self, then writing to paper, then retalk again to librarian, then write again.\n",
    "\n",
    "at1:09:30\n",
    "!!! We need to have an AI DATA MODEL map\n",
    "where we map out the AI conversations with respect to mapping to different specialist databases, human prompts, - its a conversation web modelling\n",
    "\n",
    "\n",
    "at 1:12:12\n",
    "wrapup:\n",
    "nice video on RAG, will have to use whisper for youtube\n",
    "FUTURE: can even consider other data inputs\n",
    " images (we have)\n",
    "sound + video frames\n",
    "UI: how? screenshot? video download? links?\n",
    "AI DATA MODEL:\n",
    "Process flow of conversation with Librarian and User-\n",
    "prompt engineering to meet expected outputs (QA)\n",
    "How to model conversations effectively (with constraints on COST/SPEED OF QUERY)\n",
    "optimisation? graph theory? or even use a Graph convolutional network on top of all the web to optimise conversation path and prompt engineering?\n",
    "this is athena i think and all those other company internal AI for boosting efficiency\n",
    "\n",
    "\n",
    "Post video: \n",
    "IDEA:\n",
    "lol if a RAG can do exactly my Youtube Keynotes taking here with analyses,\n",
    "analysts will be hyperturbocharged liao\n",
    "but yea depends on specialist database. so ultimately needs an SME of SME, or a specialist of the specialist database to curate this database. but yea can go even higher dimensions, reach all fields, to reach true GAI\n",
    "Which is what LLM+search engines (bing copilot) aspire to do? try to be the new google across all domains, with references,\n",
    "but not for just for information querying but for user experience, like Samsung AI\n",
    "which is making user interface more seamless with AI augmentations.\n",
    "\n",
    "so UI UX side, we should focus on what samsung is doing:\n",
    "these are the AI funnels:\n",
    "\n",
    "\"You\n",
    "galaxy ai features\n",
    "Copilot\n",
    "Certainly! Here are some of the features of Galaxy AI:\n",
    "\n",
    "Circle to Search: This feature allows you to draw a circle around any object in an image, and Galaxy AI will search for relevant information related to that object.\n",
    "Browsing Assist: Galaxy AI can assist with web browsing by summarizing articles, providing relevant links, and answering questions based on web content.\n",
    "Transcript Assist: When you upload an audio or video file, Galaxy AI can transcribe the spoken content and provide a text-based summary.\n",
    "Photo Assist: Upload an image, and Galaxy AI can describe its contents, identify objects, and provide context.\n",
    "Generative Edit: Galaxy AI can help generate creative content, such as poems, stories, code, and more.\n",
    "Call Assist/Live Translate: During phone calls, Galaxy AI can provide real-time translation or assist with conversation summaries.\n",
    "Chat Assist: Engage in text-based conversations with Galaxy AI, which can respond contextually and provide helpful information.\n",
    "Note Assist: Galaxy AI can summarize notes, generate outlines, and assist with organizing information.\"\n",
    "\n",
    "then formulate all of the thinking (and whatever lacking) behind my chats above into a RAG - then we have an AI business analyst LOL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keynotes_log = \"\"\"\n",
    "Key Notes:\n",
    "at 35:34\n",
    "text Splitting has to be done by relevance ->embeddings_search --> topN -> chain back to LLM --> talk to AI (+relevance)\n",
    "\n",
    "at 41:30\n",
    "Okay nice, he is explanining RAG nicely :\n",
    "my thoughts at that timestamp:\n",
    "TECH LEVELS:\n",
    "1) image + text  -> text \n",
    "    - No context: LLM only\n",
    "    - eg; LLM(\"Hey AI, give me information on X\")\n",
    "2) image + text -> text  \n",
    "    - Basic Contextualist: Context sample + LLM\n",
    "    - eg; LLM(\"Hey AI, give me information on X, given (Context sample)\")\n",
    "3) image + text -> text \n",
    "    - Big Contextualist: Context Population + Recommendation engine + LLM\n",
    "    - eg; RAG(\"Hey AI, give me information on X\", Librarian(topN, X, Context Population))\n",
    "\n",
    "at44:31, yes nice explanation of dimensionality reduction\n",
    "\n",
    "\n",
    "at 46:55: have to classify all avialable LLMs by\n",
    "matching capabilities, (based on Baijun's picture for example, CLAUDE has best reasoning --> does this imply better model embeddings?)\n",
    "COST (token, contextualising cost)\n",
    "\n",
    "\n",
    "at 51:38:\n",
    "macthing parameters (similiarity)\n",
    "cosine most popular, but could there be other techiques could have better edge at certain data?? (to investigate)\n",
    "\n",
    "\n",
    "\n",
    "at 53:50:\n",
    "VECTOR STORE\n",
    "db for embeddings/vectors\n",
    "to store all documents, auto gen emebddings, store auto\n",
    "!!! Vector stores are optimised to do similiarity really WELL\n",
    "sure anot, what optimisation, should we built everything inhouse to not be dependant. all this just funnels to get to pay for pinecone only. but yea gotta check cost-benefit here\n",
    "i've already make everything up till 56mins from scratch already, we already have.\n",
    "just maybe vector stores. must compare pricing and efficacy of available vector stores\n",
    "\n",
    "\n",
    "at 59:04:\n",
    "VECTOR STORE features\n",
    "vector_store.as_retriever\n",
    "connects directly to vector_store to get topN\n",
    "langchain is a wrapper for LLMs. but gemini already has this feature built in.\n",
    "scope: we aggregate all this LLM services. but remain portable and Microservices oriented as any one of them cld be bought over by corps (no new updates), etc\n",
    "\n",
    "61:09 onwards is dry pinecone stuff\n",
    "but yea could have features must replicate/generalise\n",
    "\n",
    "at 1:01:33\n",
    "LangChain codebase/structure highlights\n",
    "prompts themselves are CLASSES, which are modularly attached to RETRIEVER_CLASS\n",
    "whereas mine is functions only. Need persistence!!\n",
    "\n",
    "at1:04:20:\n",
    "langchain codebase highlights\n",
    "vector_stores are wrapped in highly functionable classes\n",
    "whereas mine is all objects\n",
    "\n",
    "\n",
    "at 1:06:46:\n",
    "Langchain UX experience and ease of \"chaining\":\n",
    "(context + prompt + model + parser ) + prompt\n",
    "so its an infinite way to arrange this.\n",
    "this is basically modelling conversations with a librarian, and then to self, then writing to paper, then retalk again to librarian, then write again.\n",
    "\n",
    "at1:09:30\n",
    "!!! We need to have an AI DATA MODEL map\n",
    "where we map out the AI conversations with respect to mapping to different specialist databases, human prompts, - its a conversation web modelling\n",
    "\n",
    "\n",
    "at 1:12:12\n",
    "wrapup:\n",
    "nice video on RAG, will have to use whisper for youtube\n",
    "FUTURE: can even consider other data inputs\n",
    " images (we have)\n",
    "sound + video frames\n",
    "UI: how? screenshot? video download? links?\n",
    "AI DATA MODEL:\n",
    "Process flow of conversation with Librarian and User-\n",
    "prompt engineering to meet expected outputs (QA)\n",
    "How to model conversations effectively (with constraints on COST/SPEED OF QUERY)\n",
    "optimisation? graph theory? or even use a Graph convolutional network on top of all the web to optimise conversation path and prompt engineering?\n",
    "this is athena i think and all those other company internal AI for boosting efficiency\n",
    "\n",
    "\n",
    "Post video: \n",
    "IDEA:\n",
    "lol if a RAG can do exactly my Youtube Keynotes taking here with analyses,\n",
    "analysts will be hyperturbocharged liao\n",
    "but yea depends on specialist database. so ultimately needs an SME of SME, or a specialist of the specialist database to curate this database. but yea can go even higher dimensions, reach all fields, to reach true GAI\n",
    "Which is what LLM+search engines (bing copilot) aspire to do? try to be the new google across all domains, with references,\n",
    "but not for just for information querying but for user experience, like Samsung AI\n",
    "which is making user interface more seamless with AI augmentations.\n",
    "\n",
    "so UI UX side, we should focus on what samsung is doing:\n",
    "these are the AI funnels:\n",
    "\n",
    "\"You\n",
    "galaxy ai features\n",
    "Copilot\n",
    "Certainly! Here are some of the features of Galaxy AI:\n",
    "\n",
    "Circle to Search: This feature allows you to draw a circle around any object in an image, and Galaxy AI will search for relevant information related to that object.\n",
    "Browsing Assist: Galaxy AI can assist with web browsing by summarizing articles, providing relevant links, and answering questions based on web content.\n",
    "Transcript Assist: When you upload an audio or video file, Galaxy AI can transcribe the spoken content and provide a text-based summary.\n",
    "Photo Assist: Upload an image, and Galaxy AI can describe its contents, identify objects, and provide context.\n",
    "Generative Edit: Galaxy AI can help generate creative content, such as poems, stories, code, and more.\n",
    "Call Assist/Live Translate: During phone calls, Galaxy AI can provide real-time translation or assist with conversation summaries.\n",
    "Chat Assist: Engage in text-based conversations with Galaxy AI, which can respond contextually and provide helpful information.\n",
    "Note Assist: Galaxy AI can summarize notes, generate outlines, and assist with organizing information.\"\n",
    "\n",
    "then formulate all of the thinking (and whatever lacking) behind my chats above into a RAG - then we have an AI business analyst LOL\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anthropicNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading anthropic-0.21.3-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from anthropic) (4.3.0)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from anthropic) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from anthropic) (2.6.3)\n",
      "Requirement already satisfied: sniffio in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from anthropic) (1.3.1)\n",
      "Collecting tokenizers>=0.13.0 (from anthropic)\n",
      "  Using cached tokenizers-0.15.2-cp312-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from anthropic) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic) (3.6)\n",
      "Requirement already satisfied: certifi in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (2.16.3)\n",
      "Collecting huggingface_hub<1.0,>=0.16.4 (from tokenizers>=0.13.0->anthropic)\n",
      "  Using cached huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2024.3.1)\n",
      "Requirement already satisfied: requests in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (23.2)\n",
      "Requirement already satisfied: colorama in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\pycharmprojects\\adam-main\\.venv\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.2.1)\n",
      "Downloading anthropic-0.21.3-py3-none-any.whl (851 kB)\n",
      "   ---------------------------------------- 0.0/851.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 851.6/851.6 kB 27.1 MB/s eta 0:00:00\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached tokenizers-0.15.2-cp312-none-win_amd64.whl (2.2 MB)\n",
      "Using cached huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Installing collected packages: distro, huggingface_hub, tokenizers, anthropic\n",
      "Successfully installed anthropic-0.21.3 distro-1.9.0 huggingface_hub-0.21.4 tokenizers-0.15.2\n"
     ]
    }
   ],
   "source": [
    "pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_01U9bW8ArZRvLpXF9sbz36DC', content=[ContentBlock(text='To build a Retrieval Augmented Generation (RAG) system with Claude, you can follow these steps:\\n\\n1. Prepare your data: Organize your proprietary data into a structured format, such as a DataFrame or a database.\\n\\n2. Embed the data: Use a suitable embedding technique to convert your data into vector representations. This allows for efficient similarity search and retrieval.\\n\\n3. Implement a retrieval function: Create a function that takes a query and retrieves the most relevant data points from your embedded data based on similarity.\\n\\n4. Integrate with Claude: Pass the retrieved data points along with the query to Claude for generating an informed response.\\n\\nHere\\'s an example function that embeds a DataFrame of data and finds the top_n most similar data points using Claude:\\n\\n```python\\nimport numpy as np\\nfrom sklearn.metrics.pairwise import cosine_similarity\\n\\ndef retrieve_top_n(query, dataframe, top_n=5):\\n    # Embed the query using Claude\\'s embedding functionality\\n    query_embedding = claude.embed(query)\\n\\n    # Embed the data in the DataFrame\\n    data_embeddings = []\\n    for _, row in dataframe.iterrows():\\n        text = \" \".join(str(value) for value in row.values)\\n        embedding = claude.embed(text)\\n        data_embeddings.append(embedding)\\n\\n    # Convert data embeddings to a numpy array\\n    data_embeddings = np.array(data_embeddings)\\n\\n    # Calculate cosine similarity between the query embedding and data embeddings\\n    similarities = cosine_similarity(query_embedding.reshape(1, -1), data_embeddings)\\n\\n    # Get the indices of the top_n most similar data points\\n    top_indices = similarities.argsort()[0][-top_n:][::-1]\\n\\n    # Retrieve the top_n most similar data points from the DataFrame\\n    top_data_points = dataframe.iloc[top_indices]\\n\\n    return top_data_points\\n```\\n\\nIn this function:\\n\\n1. The `query` is embedded using Claude\\'s embedding functionality (`claude.embed`). This assumes that you have already set up the integration with Claude.\\n\\n2. The data in the `dataframe` is embedded by iterating over each row, concatenating the values into a single text string, and embedding that string using Claude.\\n\\n3. The cosine similarity between the query embedding and each data embedding is calculated using `cosine_similarity` from scikit-learn.\\n\\n4. The indices of the `top_n` most similar data points are obtained by sorting the similarities in descending order and taking the top `top_n` indices.\\n\\n5. The `top_n` most similar data points are retrieved from the DataFrame using the obtained indices.\\n\\nYou can call this function with a query and your DataFrame to retrieve the top_n most similar data points.\\n\\nNote: Make sure to replace `claude.embed` with the actual embedding functionality provided by Claude, as the specifics may vary depending on the Claude API or library you are using.\\n\\nOnce you have retrieved the top data points, you can pass them along with the query to Claude for generating an informed response based on the retrieved context.', type='text')], model='claude-3-opus-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=74, output_tokens=709))\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "from settings import CLAUDE_API_KEY\n",
    "client = anthropic.Client(api_key=CLAUDE_API_KEY)\n",
    "system_prompt = \"\"\"\"\n",
    "You are an AI data engineer with expertise in building Retrieval Augmented Generation (RAG) for businesses to leverage their proprietary data.\n",
    "\"\"\"\n",
    "def prompt(content,\n",
    "           system_prompt,\n",
    "           max_tokens = 1024,\n",
    "           model = \"claude-3-opus-20240229\",):\n",
    "    response = client.messages.create(\n",
    "        max_tokens = max_tokens,\n",
    "        model=model,\n",
    "        system=system_prompt, # <-- system prompt\n",
    "        messages=[{\"role\": \"user\", \"content\": content}] # <-- user prompt\n",
    "    )\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "response = prompt(\"What is the best way to build a RAG system with Claude? Show me firstly a function that embeds a dataframe of data and find top_n using Claude\", system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentBlock(text='To build a Retrieval Augmented Generation (RAG) system with Claude, you can follow these steps:\\n\\n1. Prepare your data: Organize your proprietary data into a structured format, such as a DataFrame or a database.\\n\\n2. Embed the data: Use a suitable embedding technique to convert your data into vector representations. This allows for efficient similarity search and retrieval.\\n\\n3. Implement a retrieval function: Create a function that takes a query and retrieves the most relevant data points from your embedded data based on similarity.\\n\\n4. Integrate with Claude: Pass the retrieved data points along with the query to Claude for generating an informed response.\\n\\nHere\\'s an example function that embeds a DataFrame of data and finds the top_n most similar data points using Claude:\\n\\n```python\\nimport numpy as np\\nfrom sklearn.metrics.pairwise import cosine_similarity\\n\\ndef retrieve_top_n(query, dataframe, top_n=5):\\n    # Embed the query using Claude\\'s embedding functionality\\n    query_embedding = claude.embed(query)\\n\\n    # Embed the data in the DataFrame\\n    data_embeddings = []\\n    for _, row in dataframe.iterrows():\\n        text = \" \".join(str(value) for value in row.values)\\n        embedding = claude.embed(text)\\n        data_embeddings.append(embedding)\\n\\n    # Convert data embeddings to a numpy array\\n    data_embeddings = np.array(data_embeddings)\\n\\n    # Calculate cosine similarity between the query embedding and data embeddings\\n    similarities = cosine_similarity(query_embedding.reshape(1, -1), data_embeddings)\\n\\n    # Get the indices of the top_n most similar data points\\n    top_indices = similarities.argsort()[0][-top_n:][::-1]\\n\\n    # Retrieve the top_n most similar data points from the DataFrame\\n    top_data_points = dataframe.iloc[top_indices]\\n\\n    return top_data_points\\n```\\n\\nIn this function:\\n\\n1. The `query` is embedded using Claude\\'s embedding functionality (`claude.embed`). This assumes that you have already set up the integration with Claude.\\n\\n2. The data in the `dataframe` is embedded by iterating over each row, concatenating the values into a single text string, and embedding that string using Claude.\\n\\n3. The cosine similarity between the query embedding and each data embedding is calculated using `cosine_similarity` from scikit-learn.\\n\\n4. The indices of the `top_n` most similar data points are obtained by sorting the similarities in descending order and taking the top `top_n` indices.\\n\\n5. The `top_n` most similar data points are retrieved from the DataFrame using the obtained indices.\\n\\nYou can call this function with a query and your DataFrame to retrieve the top_n most similar data points.\\n\\nNote: Make sure to replace `claude.embed` with the actual embedding functionality provided by Claude, as the specifics may vary depending on the Claude API or library you are using.\\n\\nOnce you have retrieved the top data points, you can pass them along with the query to Claude for generating an informed response based on the retrieved context.', type='text')]\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(input_tokens=71, output_tokens=355)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "with client.messages.stream(\n",
    "    max_tokens=1024,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "    model=\"claude-3-opus-20240229\",\n",
    ") as stream:\n",
    "  for text in stream.text_stream:\n",
    "      print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KnowledgeBase - Financial Data Time Series\n",
    "LLM image analysis --> analyse_travel_destination --> Concierge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) PROMPT input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"2024-03-04\"\n",
    "prompt_0 = f\"\"\"\n",
    "What is the current price of Bitcoin for today ({date})? What is the latest financial news, outlook on the crypto market, \n",
    "and also contrast with the traditional stock market and money market.\n",
    "\"\"\"\n",
    "from settings import GEMINI_API_KEY\n",
    "from llm_handler.GHandler import GHandler\n",
    "\n",
    "g_handler = GHandler(GEMINI_API_KEY)\n",
    "\n",
    "response = g_handler.prompt(prompt_0)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Image input: Multimodal LLM image analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = \"./database/quant/btcusdt_1d_IST.png\"\n",
    "image_path = \"./database/quant/buddy_chinese.png\"\n",
    "\n",
    "prompt_1 = f\"\"\"\n",
    "            you are an expert AI generated image analyst- in a AI ethics and misuse department.\n",
    "            You are given images and asked to analyse them if it is AI generated or not.\n",
    "            And give your reasonings.\n",
    "            \"\"\"\n",
    "prompt_2 = f\"\"\"If the image is AI generated, provide a confidence score from 0 to 100,\n",
    "            and recreate the prompt that generated the image. \n",
    "            If the image is not AI generated, then craft a single paragraph, descriptive prompt that could result in a similar image.\n",
    "\"\"\"\n",
    "# prompt_2v = f\"\"\"If the image is AI generated, provide a confidence score from 0 to 100,\n",
    "# \"The expert AI generated image analyst response to the prior prompt ({prompt_1}) is: \n",
    "# <trader's RESPONSE>\".\n",
    "# \"My own thoughts on his response is: \n",
    "# <vibe_checker's RESPONSE>\".\n",
    "# \"Confidence: <YOUR_SCORE>\".\n",
    "# \"\"\"\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=image_path))\n",
    "from pprint import pprint\n",
    "from settings import GEMINI_API_KEY\n",
    "from llm_handler.GHandler import GHandler\n",
    "\n",
    "g_handler = GHandler(GEMINI_API_KEY)\n",
    "\n",
    "g_response = g_handler.prompt_image(model_name = \"gemini-pro-vision\",\n",
    "                                  image_path = image_path,\n",
    "                                  prompt_1 = prompt_1,\n",
    "                                  prompt_2 = prompt_2,\n",
    "                                  generation_config = {\n",
    "                                                        \"temperature\": 0.9,\n",
    "                                                        \"top_p\": 0.95,\n",
    "                                                        \"top_k\": 40,\n",
    "                                                        \"max_output_tokens\": 1024,\n",
    "                                                        },\n",
    "                                block_threshold = \"BLOCK_NONE\"\n",
    "                                        )\n",
    "# print this in a nicer format and not stretched beyond the screen\n",
    "pprint(g_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Generate an image with the following description: \n",
    " \"The photograph is of a gray tabby cat being held by a person. The cat's eyes \"\n",
    " 'are partially closed and it looks relaxed. The photograph is taken from a '\n",
    " 'close-up angle and the background is out of focus.\\n'\n",
    " '\\n'\n",
    " 'To take a similar photograph, you would need a camera and a cat. You would '\n",
    " 'need to get close to the cat and take the photograph when it is relaxed. You '\n",
    " 'could also use a telephoto lens to get a closer shot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet you provided attempts to utilize the Gemini-Pro-Vision model for image analysis in the context of a financial time series chart. However, there are several limitations and concerns to consider:\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "1. **Model capabilities:** While Gemini-Pro-Vision is trained on a massive dataset of text and code, it isn't specifically designed for analyzing financial charts. Its ability to interpret the image and provide the specific descriptions, diagnoses, predictions, and prescriptions you require might be limited.\n",
    "\n",
    "2. **Data complexity:** Financial charts contain complex information encoded in lines, colors, and patterns. Capturing this information and translating it into accurate financial insights requires specialized training data and models.\n",
    "\n",
    "**Concerns:**\n",
    "\n",
    "1. **Over-reliance on LLM:** The code fully relies on the LLM's interpretation of the image, leaving no room for human expertise or domain knowledge. This can lead to inaccurate or misleading financial insights.\n",
    "\n",
    "2. **Ethical considerations:** Using an LLM for financial predictions without proper validation and transparency can be risky. It's essential to be clear about the limitations of the system and emphasize that its output should not be solely relied upon for investment decisions.\n",
    "\n",
    "3. **Potential misuse:** This approach might encourage users to interpret the LLM's output as financial advice, even though it might lack the necessary accuracy and context.\n",
    "\n",
    "**Alternative Approach:**\n",
    "\n",
    "Instead of relying solely on an LLM, consider a more **hybrid approach**:\n",
    "\n",
    "1. **Human Analysis:** Involve a human financial analyst to analyze the chart and provide their expertise. This can help ensure the accuracy and reliability of the initial assessment.\n",
    "\n",
    "2. **LLM Support:** Use the LLM to **complement the human analysis** by:\n",
    "    * Summarizing relevant financial news or historical data related to the instrument.\n",
    "    * Providing alternative perspectives or highlighting potential risks.\n",
    "    * Generating different creative writing formats (e.g., a news article about the market situation) based on the analyst's insights.\n",
    "\n",
    "This approach leverages the strengths of both humans and LLMs, potentially leading to more robust and reliable insights while mitigating ethical concerns.\n",
    "\n",
    "It's crucial to remember that LLMs are still under development, and their capabilities in specialized domains like finance are evolving. While they can be valuable tools, exercising caution and combining them with human expertise remains essential, especially when dealing with financial decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Inventory Recommendation Engine\n",
    "\n",
    "\n",
    "i) Embed database\n",
    "\n",
    "ii) embed user_travel_LLM_response above\n",
    "\n",
    "iii) AI recommendation engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Embed -> Recommend Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"./database/travel/Data Model for Travel.xlsx\"\n",
    "sheet_name = \"Day Trip\"\n",
    "\n",
    "from settings import GEMINI_API_KEY\n",
    "from llm_handler import GHandler\n",
    "import importlib\n",
    "importlib.reload(GHandler)\n",
    "from llm_handler.GHandler import GHandler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "day_trips_df = pd.read_excel(db_path, sheet_name=sheet_name)\n",
    "# need to embed the day_trips_df\n",
    "day_trips_df[\"Text\"] = day_trips_df.apply(lambda row: f\"Activity: {row['Activity']}, Location: {row['Location']}, Category: {row['Category']} Price: {row['Price']}\", axis=1)\n",
    "\n",
    "\n",
    "g_handler = GHandler(GEMINI_API_KEY)\n",
    "df_embedded = g_handler.embed_df(day_trips_df,\n",
    "                                 title = \"Activity\", \n",
    "                                 text = \"Text\",\n",
    "                                 model=\"models/embedding-001\")\n",
    "daytrip_recommendation = g_handler.find_best_passage(g_response.text, df_embedded)\n",
    "print(daytrip_recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Cascade Recommendation Engine\n",
    "\n",
    "Given the user's primary match, the recommendation engine will look up secondary and tertiary recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for the hotels and flights\n",
    "flights_hotels_df = pd.read_excel(r'./database/travel/Data Model for Travel.xlsx', sheet_name=\"Hotels\")\n",
    "# drop where type == Nan\n",
    "flights_hotels_df = flights_hotels_df.dropna(subset=[\"Type\"])\n",
    "flights_hotels_df[\"Text\"] = flights_hotels_df.apply(lambda row: f\"Name: {row['Name']}, Type: {row['Type']}, Location: {row['Location']}, Price: {row['Price']}, Description: {row['Description']}\", axis=1)\n",
    "\n",
    "\n",
    "g_handler = GHandler(GEMINI_API_KEY)\n",
    "flights_hotels_df = g_handler.embed_df(flights_hotels_df,\n",
    "                                 title = \"Name\", \n",
    "                                 text = \"Text\",\n",
    "                                 model=\"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Convo_2 = f\"\"\"I have an inventory for a {daytrip_recommendation}. I can recommend you a more luxurious based on the location of the activity.\"\"\"\n",
    "hotel_recommendation = g_handler.find_best_passage(Convo_2, flights_hotels_df)\n",
    "print(hotel_recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Convo_3 = f\"\"\"I have an inventory for a {daytrip_recommendation}. I can recommend you the longest flight based on the location of the activity.\"\"\"\n",
    "flight_recommendation = g_handler.find_best_passage(Convo_3, flights_hotels_df)\n",
    "print(flight_recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_hotels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper that inserts\n",
    "# daytrip + hotel + flight into a 3 day 2 night itenerary\n",
    "# so first day fill with casual eating and sightseeing, then 2nd day is the daytrip focus, and 3rd day is the return flight\n",
    "# LLM has to be smart enough to infer duration of daytrip (hiking mt rinjani is 2 days, so it should be 2 days 1 night)\n",
    "\n",
    "# Eg: himalayas\n",
    "# has to account for weather, seasons, and other factors that might affect the trip duration. Or postpone trip under more optimal factors\n",
    "# TESTCASE: NO inventory for customer request\n",
    "# TESTCASE: transport types (ferries, etc)\n",
    "\n",
    "# look at all factor buckets that involves inventory \n",
    "# Buckets to quantify the inventory\n",
    "# main factor is affiliation, services (driver, villa-cook, guide, translator), then location, then price, then type, then rating, then availability, then duration, then weather, then season, then other factors)\n",
    "# if not affiliated then shouldnt be recommended \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top n recommendations (pro consumer vs pro business)\n",
    "\n",
    "# need to see if they have existing monetisation strategy for their travel packages\n",
    "# recommmendations should be tailored to their monetisation strategy\n",
    "# (eg: if they have a partnership with a hotel, then recommend that hotel)\n",
    "\n",
    "\n",
    "# main objective: recommendation engine \n",
    "# warning: no buckets for existing monetisation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daytrip_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UX_prompt = f\"\"\"\n",
    "Generate a travel package for the given trip_recommendation:\n",
    "{daytrip_recommendation}\n",
    "\n",
    "This trip_recommendation comes with the following hotel recommendation:\n",
    "{hotel_recommendation}\n",
    "\n",
    "This trip_recommendation comes with the following flight recommendation:\n",
    "{flight_recommendation}\n",
    "\n",
    "The package should include the following sections:\n",
    "\n",
    "\"Summary\"\n",
    "introductory and summary of the trip in one paragraph.\n",
    "The summary should describe in vivid detail, the main attractions, activities, and experiences that the travelers can enjoy in the trip_recommendation. \n",
    "\n",
    "\"Journey Highlights\"\n",
    "A list of the main features and most exciting aspects of the package.\n",
    "the highlights must end off with a bold line: \"Your journey takes you to: x - y - z\"\n",
    "\n",
    "\"Itinerary & Map\" \n",
    "This section is an itenerary list that shows the day-by-day plan of the trip, that is also accompanied by a map.\n",
    "\n",
    "The itinerary should include the name, location, and description of each place or activity that the travelers will visit or do each day. \n",
    "The itinerary should also indicate the approximate duration and transportation mode for each item.\n",
    "\n",
    "A highlights and inclusions section that lists the main features and benefits of the package. \n",
    "The section should mention what is included in the price, such as flights, accommodation, meals, guides, entrance fees, etc. \n",
    "The section should also mention any special offers or discounts that are available for the package.\n",
    "A dates and pricing section that shows the available dates and prices for the package. \n",
    "The section should indicate the departure and return dates, the number of travelers, the total cost, and the payment options for the package. \n",
    "The section should also provide a link or contact information for booking or inquiring about the package.\n",
    "\n",
    "All information derived here should be based on the recommendations from the previous steps and MUST not be fabricated.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import GEMINI_API_KEY\n",
    "from llm_handler.GHandler import GHandler\n",
    "\n",
    "g_handler = GHandler(GEMINI_API_KEY)\n",
    "\n",
    "response_UX = g_handler.prompt(UX_prompt)\n",
    "print(response_UX.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format mimic from image of travel website package page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./database/travel/lux_example.jpg\"\n",
    "prompt_1 = f\"\"\"Reproduce the format of this travel package given a different destination: \n",
    "Generate a travel package for the given trip_recommendation:\n",
    "{daytrip_recommendation}\n",
    "\n",
    "This trip_recommendation comes with the following hotel recommendation:\n",
    "{hotel_recommendation}\n",
    "\n",
    "This trip_recommendation comes with the following flight recommendation:\n",
    "{flight_recommendation}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import Image, display\n",
    "# display(Image(filename=image_path))\n",
    "\n",
    "from settings import GEMINI_API_KEY\n",
    "from llm_handler.GHandler import GHandler\n",
    "\n",
    "g_handler = GHandler(GEMINI_API_KEY)\n",
    "\n",
    "g_response = g_handler.prompt_image(model_name = \"gemini-pro-vision\",\n",
    "                                  image_path = image_path,\n",
    "                                  prompt_1 = prompt_1,\n",
    "                                  prompt_2 = None)\n",
    "print(g_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Full Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./database/travel/me-at-kelingking-beach-nusa-penida-bali-inonesia-laugh-traveleat.jpg\"\n",
    "prompt_1 = \"Tell me the location where this photo is taken from?\"\n",
    "prompt_2 = \"Based on the response, recommend a full day trip travel itinerary\"\n",
    "from IPython.display import Image, display\n",
    "print(\"(a) Image Analyses\")\n",
    "display(Image(filename=image_path))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from settings import GEMINI_API_KEY\n",
    "from llm_handler.GHandler import GHandler\n",
    "\n",
    "g_handler = GHandler(GEMINI_API_KEY)\n",
    "\n",
    "g_response = g_handler.prompt_image(model_name = \"gemini-pro-vision\",\n",
    "                                  image_path = image_path,\n",
    "                                  prompt_1 = prompt_1,\n",
    "                                  prompt_2 = prompt_2)\n",
    "# print(g_response.text)\n",
    "print(\"(b) Image Analyses Complete\")\n",
    "\n",
    "db_path = \"./database/travel/Data Model for Travel.xlsx\"\n",
    "sheet_name = \"Day Trip\"\n",
    "\n",
    "def get_day_trip_recommendation(g_response, db_path, sheet_name):\n",
    "    day_trips_df = pd.read_excel(db_path, sheet_name=sheet_name)\n",
    "    # need to embed the day_trips_df\n",
    "    day_trips_df[\"Text\"] = day_trips_df.apply(lambda row: f\"Activity: {row['Activity']}, Location: {row['Location']}, Category: {row['Category']} Price: {row['Price']}\", axis=1)\n",
    "\n",
    "\n",
    "    g_handler = GHandler(GEMINI_API_KEY)\n",
    "    df_embedded = g_handler.embed_df(day_trips_df,\n",
    "                                    title = \"Activity\", \n",
    "                                    text = \"Text\",\n",
    "                                    model=\"models/embedding-001\")\n",
    "    daytrip_recommendation = g_handler.find_best_passage(g_response.text, df_embedded)\n",
    "    return daytrip_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"./database/travel/Data Model for Travel.xlsx\"\n",
    "sheet_name = \"Day Trip\"\n",
    "\n",
    "from settings import GEMINI_API_KEY\n",
    "from llm_handler import GHandler\n",
    "import importlib\n",
    "importlib.reload(GHandler)\n",
    "from llm_handler.GHandler import GHandler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "day_trips_df = pd.read_excel(db_path, sheet_name=sheet_name)\n",
    "# need to embed the day_trips_df\n",
    "day_trips_df[\"Text\"] = day_trips_df.apply(lambda row: f\"Activity: {row['Activity']}, Location: {row['Location']}, Category: {row['Category']} Price: {row['Price']}\", axis=1)\n",
    "\n",
    "\n",
    "g_handler = GHandler(GEMINI_API_KEY)\n",
    "df_embedded = g_handler.embed_df(day_trips_df,\n",
    "                                 title = \"Activity\", \n",
    "                                 text = \"Text\",\n",
    "                                 model=\"models/embedding-001\")\n",
    "daytrip_recommendation = g_handler.find_best_passage(g_response.text, df_embedded)\n",
    "print(daytrip_recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk to your Fitra AI \n",
    "\n",
    "So imagine you are walking back home, from work, on your phone, \n",
    "Then you see a website with some inspiring duas, and you want to analyse and \n",
    "understand the meaning of the duas, and you want to know the meaning of the duas,\n",
    "and find contextually similiar duas or information. \n",
    "eg;\n",
    "- \"Rabbana atina fid-dunya hasanatan wa fil 'akhirati hasanatan waqina 'adhaban-nar\"\n",
    "- \"Our Lord, give us in this world [that which is] good and in the Hereafter [that which is] good and protect us from the punishment of the Fire.\"\n",
    "- then you want to find similar duas, or background information about this dua like its origins, hadith chains, and all that \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TECH LEVELS:\n",
    "1) image + text  -> text \n",
    "    - No context: LLM only\n",
    "    - eg; LLM(\"Hey AI, give me information on X\")\n",
    "2) image + text -> text  \n",
    "    - Basic Contextualist: Context sample + LLM\n",
    "    - eg; LLM(\"Hey AI, give me information on X, given (Context sample)\")\n",
    "3) image + text -> text \n",
    "    - Big Contextualist: Context Population + Recommendation engine + LLM\n",
    "    - eg; RAG(\"Hey AI, give me information on X\", Librarian(topN, X, Context Population))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from specialists.muslim import Muslim \n",
    "muslim = Muslim()\n",
    "muslim.load_data_model(reembed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = muslim.tables[\"Sunan al Tirmidhi\"].iloc[1,1]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= muslim.model_specialist.embed_text(title= \"hadith\", text= text, model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from llm_handler.GHandler import GHandler\n",
    "from settings import GEMINI_API_KEY\n",
    "\n",
    "g_handler = GHandler(GEMINI_API_KEY)\n",
    "\n",
    "db_path = \"./database/muslim/Sunan al Tirmidhi.csv\"\n",
    "df = pd.read_csv(db_path)\n",
    "df.iloc[0]\n",
    "\n",
    "#  translate df.iloc[0] from arabic to english\n",
    "response = g_handler.prompt(f\"Please translate the following from arabic to english: It is from a hadith: {df.iloc[0,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech tier I: Basic Contextualist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = \"./database/quant/btcusdt_1d_IST.png\"\n",
    "image_path = \"./database/Fitra AI/dua_1.png\"\n",
    "\n",
    "prompt_1 = f\"\"\"\n",
    "            You are a curator and specialist of all knowledge regarding Islam- \n",
    "            in a AI ethics and misuse department.\n",
    "            You are given images and asked to analyse them for their Islamic content and also level of AI influence.\n",
    "            So look firstly, for Islamic data authenticity then after that, if there are any AI generated content,\n",
    "            especially in meanings or translations, give your reasonings.\n",
    "            \"\"\"\n",
    "prompt_2 = f\"\"\"\n",
    "            If the image is not AI generated and is authentic Islamic content, \n",
    "            then suggest a list of relevant topics to the Islamic content in this image,\n",
    "            such as its origins, hadith sources, Quran sources, and other relevant Islamic topics.\n",
    "\"\"\"\n",
    "# prompt_2v = f\"\"\"If the image is AI generated, provide a confidence score from 0 to 100,\n",
    "# \"The expert AI generated image analyst response to the prior prompt ({prompt_1}) is: \n",
    "# <trader's RESPONSE>\".\n",
    "# \"My own thoughts on his response is: \n",
    "# <vibe_checker's RESPONSE>\".\n",
    "# \"Confidence: <YOUR_SCORE>\".\n",
    "# \"\"\"\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=image_path))\n",
    "from pprint import pprint\n",
    "from settings import GEMINI_API_KEY\n",
    "from llm_handler.GHandler import GHandler\n",
    "\n",
    "g_handler = GHandler(GEMINI_API_KEY,\n",
    "                     generation_config = {\"temperature\": 0.9,\n",
    "                                      \"top_p\": 0.95,\n",
    "                                      \"top_k\": 40,\n",
    "                                      \"max_output_tokens\": 1024,\n",
    "                                      },\n",
    "                     block_threshold=\"BLOCK_NONE\",\n",
    "                    )\n",
    "\n",
    "g_response = g_handler.prompt_image(model_name = \"gemini-pro-vision\",\n",
    "                                  image_path = image_path,\n",
    "                                  prompt_1 = prompt_1,\n",
    "                                  prompt_2 = prompt_2,)\n",
    "# print this in a nicer format and not stretched beyond the screen\n",
    "pprint(g_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech tier II: Small Contextualist\n",
    "\n",
    "\n",
    "i) Embed database\n",
    "\n",
    "ii) embed user_travel_LLM_response above\n",
    "\n",
    "iii) AI recommendation engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Embed -> Recommend Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"./database/travel/SWTT_ Master Database.xlsx\"\n",
    "sheet_name = \"CUSTOMERS\"\n",
    "\n",
    "from settings import GEMINI_API_KEY\n",
    "from llm_handler import GHandler\n",
    "import importlib\n",
    "importlib.reload(GHandler)\n",
    "from llm_handler.GHandler import GHandler\n",
    "\n",
    "import pandas as pd\n",
    "tabs = pd.ExcelFile(db_path).sheet_names \n",
    "tabs = [sheet for sheet in tabs if \"TEST\" in sheet]\n",
    "print(tabs)\n",
    "# df = pd.read_excel(db_path, sheet_name=sheet_name)\n",
    "# # need to embed the day_trips_df\n",
    "# day_trips_df[\"Text\"] = day_trips_df.apply(lambda row: f\"Activity: {row['Activity']}, Location: {row['Location']}, Category: {row['Category']} Price: {row['Price']}\", axis=1)\n",
    "\n",
    "\n",
    "# g_handler = GHandler(GEMINI_API_KEY)\n",
    "# df_embedded = g_handler.embed_df(day_trips_df,\n",
    "#                                  title = \"Activity\", \n",
    "#                                  text = \"Text\",\n",
    "#                                  model=\"models/embedding-001\")\n",
    "# daytrip_recommendation = g_handler.find_best_passage(g_response.text, df_embedded)\n",
    "# print(daytrip_recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the YouTube video we're going to use.\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=OZXyYyRstMg\"\n",
    "\n",
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "\n",
    "\n",
    "# Let's do this only if we haven't created the transcription file yet.\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    youtube = YouTube(YOUTUBE_VIDEO)\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "    # Let's load the base model. This is not the most accurate\n",
    "    # model but it's fast.\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file = audio.download(output_path=tmpdir)\n",
    "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "\n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = \"\"\"\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tACCOMMODATION ARRANGMENTS\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNUMBER OF PAX\t\t\t\t\t\t\t\tTRAVEL INSURANCE\t\t\t\t\t\t\t\t\tVISA ARRANGEMENTS\t\t\t\t\t\t\t\t\tBESPOKE VIP SERVICES\t\t\t\t\t\t\tENTIRE TRIP GRAND TOTAL (MYR)\n",
    "ITEM\tCLIENTELE\t\t\t\t\t\t\t\t\t\t\t\t\tGENERAL BIODATA\t\t\t\t\t\t\t\t\tCLIENT DIRECT BOOKING\t\t\t\t\t\t\tPIC/BOOKER\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTRIP\t\t\t\t\t\t\tPURCHASE TYPE\t\t\t\t\t\t\tFLIGHT ARRANGEMENTS\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tB2B PLATFORM\tDESTINATION\tGROUP\tPROPERTY TYPE\tHOTEL BRAND\tSTAR RATING\tTRAVEL DATE\tPERIOD OF STAY (NUMBER OF NIGHTS)\tROOM CATEGORY\tROOM CONFIGURATION (BED-TYPE)\tROOM UNITS\tROOM OCCUPANCY (NUMBER OF GUESTS)\tROOM RATE WITH BREAKFAST/NIGHT (MYR)\t\tROOM-ONLY RATE /NIGHT (MYR)\t\tGRAND TOTAL\t\"ADULT \n",
    "(12 YEARS OLD AND ABOVE)\"\t\"CHILD\n",
    "(INFANT - 11 YEARS OLD)\"\t\tDOMESTIC HELPER\t\tSTAFF\tBODYGUARD\tPOLICE ESCORT\tINSURANCE COMPANY\tINSURANCE TYPE\tINSURANCE PLAN\tTRIP PLAN\tPOLICY NUMBER\tPERIOD OF COVERAGE\tCOST\tSELLING\tCOUNTRIES WE COVER\tVISA TYPE\tVISA CATEGORY\tAPPLICANT NAME\tREFERENCE NUMBER\tPERIOD OF STAY\tINSURED BY\tCOST PER PERSON\tMARK UP RATE (MYR)\tSELLING RATE PER PERSON\tMEET & GREET\tAIRPORT DUTY\tBUGGY ASSISTANCE\tLUGGAGE WRAP SERVICE\tSPECIAL CARE ASSISTANCE\tVIP LOUNGE\tSMART WORLD ALLOWANCE TO STAFF\t\n",
    "\tCLIENT ID\tORGANIZATION\tSEGMENT\tREMARK\tPREFIX\tTITLE\tFULL NAME\tDESIGNATION | OCCUPATION\tRELATIONSHIP\tCLIENT ID\tCOMPANY\tBUSINESS ADDRESS\tREGION\tBIRTH DATE\tPASSPORT NUMBER\tIDENTIFICATION NUMBER\tMARITAL STATUS\tGENDER\tAGE\tRACE\tNATIONALITY \tRELIGION\tCLIENT TYPE\t\"CLIENT SINCE\n",
    "(YEAR)\"\tEMAIL ADDRESS\tCOUNTRY CODE\tPHONE\tCOUNTRY CODE\tMOBILE\tPREFIX\tTITLE\tBOOKER NAME\tDESIGNATION/OCCUPATION\tRELATIONSHIP\tCLIENT ID\tDEPARTMENT\tCOMPANY\tBUSINESS ADDRESS\tREGION\tEMAIL ADDRESS\tCOUNTRY CODE\tPHONE\tCOUNTRY CODE\tMOBILE\tCLIENT SPENDING POWER\tPURPOSE OF TRAVEL\tTRAVEL TYPE\tOCCASION\tFAMILY COMPOSITION\t\"DATE OF TRAVEL\n",
    "(DD.MM.YY - DD.MM.YY)\"\tLENGTH OF TRAVEL (DAY)\tFLIGHT\tACCOMMODATION\tTRANSPORTATION\tVIP SERVICE\tVISA\tTRAVEL INSURANCE\tTOUR PACKAGE\tTRIP NAME\tAIRLINE NAME\tCATEGORY\tDESTINATION\tDOMESTIC FLIGHT\tINTERNATIONAL FLIGHT\tONE WAY TRIP\tRETURN TICKET\tDEPARTURE DATE\tDEPARTURE TIME\tARRIVAL DATE\tARRIVAL TIME\tCOST\tMARK UP RATE\tSELLING\t\t\t\t\t\t\t\t\t\t\t\t\tCOST ROOM RATE PER NIGHT WITH BREAKFAST (MYR)\tSELLING ROOM RATE PER NIGHT WITH BREAKFAST (MYR)\tCOST ROOM-ONLY RATE PER NIGHT (MYR)\tSELLING ROOM -ONLY RATE PER NIGHT (MYR)\t\t\t\t\tNATIONALITY\tPAX\t\t\t\t\t\t\t\t\t\t\t\tCLUSTER | DESTINATION\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./database/travel/travel_itinerary.jpg\"\n",
    "prompt_1 = \"From this picture of a travel itinerary\"\n",
    "prompt_2 = \"Based on the response, recommend a full day trip travel itinerary\"\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=image_path))\n",
    "\n",
    "from settings import GEMINI_API_KEY\n",
    "from llm_handler.GHandler import GHandler\n",
    "\n",
    "g_handler = GHandler(GEMINI_API_KEY)\n",
    "\n",
    "g_response = g_handler.prompt_image(model_name = \"gemini-pro-vision\",\n",
    "                                  image_path = image_path,\n",
    "                                  prompt_1 = prompt_1,\n",
    "                                  prompt_2 = prompt_2)\n",
    "print(g_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Process Flow\n",
    "0) customer request \n",
    "-  \n",
    "\n",
    "1) Flight booking\n",
    "- SABRE \n",
    "- This must be determined first, \n",
    "\n",
    "\n",
    "2) Hotel: B2B platforms: \n",
    "- WithinEarth\n",
    "- TBO\n",
    "- RAtehawk \n",
    "- booking.com \n",
    "\n",
    "\n",
    "tour package\n",
    "- DMC partner lag (24 - 48hours) \n",
    "\n",
    "DMC partners need to be filtered, \n",
    "\n",
    "proposal:  0.5 days, finalis, another half of the day - hanis will compile\n",
    "- check flight first, ticketing then hotel. \n",
    "STANDARD: 24-48 hours within proposal to get invoice. \n",
    "\n",
    "proposals usually take 24 hours, \n",
    "\n",
    "invoice, confirmed, wit client then draft of the invoice, --> to finance, and client. \n",
    "- depends on payment terms, if they have add on services, then invoice will be sent after trip. \n",
    "- mostly high profile boss, but no penalty. penalty 1.2% ---> 15% pay upfreont, another 10% credit term, 5% deposit, 75% the rest after \n",
    "\n",
    "\n",
    "\n",
    "mostly the same for umrah. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
