{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PycharmProjects\\adam\\venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Gemini' from 'llama_index.llms' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pinecone\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Gemini\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_stores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PineconeVectorStore\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StorageContext\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Gemini' from 'llama_index.llms' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "from llama_index.llms import Gemini\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.embeddings import GeminiEmbedding\n",
    "from llama_index import ServiceContext, VectorStoreIndex, download_loader, set_global_service_context\n",
    "from settings import GOOGLE_API_KEY, PINECONE_API_KEY\n",
    "\n",
    "import pandas as pd  # For working with DataFrames\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "index_name = \"adam\"\n",
    "DATA_URL = \"https://www.gettingstarted.ai/how-to-use-gemini-pro-api-llamaindex-pinecone-index-to-build-rag-app/\"\n",
    "\n",
    "# set llm as Gemini Pro\n",
    "llm = Gemini()\n",
    "\n",
    "# create pinecone client\n",
    "pinecone_client = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "\n",
    "# list pinecone indexes\n",
    "for index in pinecone_client.list_indexes():\n",
    "    print(index['name'])\n",
    "\n",
    "# select pinecone index 'posts'\n",
    "pinecone_index = pinecone_client.Index(index_name)\n",
    "# index_description = pinecone_client.describe_index(\"posts\")\n",
    "\n",
    "# load page using llamaindex\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=[DATA_URL])\n",
    "\n",
    "# grab embeddings from gemini embeddings model\n",
    "gemini_embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=gemini_embed_model)\n",
    "\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "# store embeddings in pinecone index\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "# # create Pinecone index programmatically \n",
    "# if input(\"re\") == \"y\":\n",
    "#     index = VectorStoreIndex.from_documents(\n",
    "#         documents, \n",
    "#         storage_context=storage_context\n",
    "#     )\n",
    "\n",
    "# query pinecone index for similar embeddings\n",
    "query_engine = index.as_query_engine()\n",
    "gemini_response = query_engine.query(\"\"\"What is the summary of the context provided\n",
    "                                     \"\"\")\n",
    "\n",
    "# print response\n",
    "import textwrap\n",
    "print(textwrap.fill(str(gemini_response), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Openai' from 'llama_index.llms' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Openai\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Openai' from 'llama_index.llms' (unknown location)"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (0.9.45)\n",
      "Collecting llama_index\n",
      "  Downloading llama_index-0.10.4-py3-none-any.whl (5.6 kB)\n",
      "Collecting llama-index-agent-openai<0.2.0,>=0.1.0 (from llama_index)\n",
      "  Downloading llama_index_agent_openai-0.1.1-py3-none-any.whl (12 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.0 (from llama_index)\n",
      "  Downloading llama_index_core-0.10.3-py3-none-any.whl (630 kB)\n",
      "                                              0.0/630.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 630.7/630.7 kB 20.0 MB/s eta 0:00:00\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.0 (from llama_index)\n",
      "  Downloading llama_index_embeddings_openai-0.1.1-py3-none-any.whl (6.1 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama_index)\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "                                              0.0/2.0 MB ? eta -:--:--\n",
      "     -----------------------------            1.5/2.0 MB 31.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 31.3 MB/s eta 0:00:00\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.0 (from llama_index)\n",
      "  Downloading llama_index_llms_openai-0.1.1-py3-none-any.whl (9.6 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 (from llama_index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.1-py3-none-any.whl (6.0 kB)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.0 (from llama_index)\n",
      "  Downloading llama_index_program_openai-0.1.1-py3-none-any.whl (4.3 kB)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.0 (from llama_index)\n",
      "  Downloading llama_index_question_gen_openai-0.1.1-py3-none-any.whl (3.1 kB)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.0 (from llama_index)\n",
      "  Downloading llama_index_readers_file-0.1.3-py3-none-any.whl (36 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.11.0,>=0.10.0->llama_index)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "                                              0.0/144.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 144.7/144.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2024.2.0)\n",
      "Requirement already satisfied: httpx in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.26.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.11.1)\n",
      "Requirement already satisfied: pandas in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2.2.0)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.11.0,>=0.10.0->llama_index)\n",
      "  Downloading pillow-10.2.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "                                              0.0/2.6 MB ? eta -:--:--\n",
      "     -----------------------                  1.5/2.6 MB 32.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 33.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.31.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.5.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (4.12.3)\n",
      "Collecting bs4<0.0.3,>=0.0.2 (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index)\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Collecting pymupdf<2.0.0,>=1.23.21 (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index)\n",
      "  Downloading PyMuPDF-1.23.22-cp311-none-win_amd64.whl (3.4 MB)\n",
      "                                              0.0/3.4 MB ? eta -:--:--\n",
      "     ------------------------------------     3.1/3.4 MB 66.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.4/3.4 MB 54.6 MB/s eta 0:00:00\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index)\n",
      "  Downloading pypdf-4.0.1-py3-none-any.whl (283 kB)\n",
      "                                              0.0/284.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 284.0/284.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.16.0)\n",
      "Requirement already satisfied: click in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (2023.12.25)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.6.1)\n",
      "Requirement already satisfied: sniffio in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.3.0)\n",
      "Requirement already satisfied: certifi in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama_index) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.0.2)\n",
      "Requirement already satisfied: idna in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama_index) (3.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.0->llama_index) (0.14.0)\n",
      "Collecting PyMuPDFb==1.23.22 (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama_index)\n",
      "  Downloading PyMuPDFb-1.23.22-py3-none-win_amd64.whl (24.5 MB)\n",
      "                                              0.0/24.5 MB ? eta -:--:--\n",
      "     --                                       1.6/24.5 MB 50.2 MB/s eta 0:00:01\n",
      "     ------                                   4.2/24.5 MB 54.0 MB/s eta 0:00:01\n",
      "     -------------                            8.0/24.5 MB 64.0 MB/s eta 0:00:01\n",
      "     ------------------                      11.8/24.5 MB 72.6 MB/s eta 0:00:01\n",
      "     -------------------------               16.2/24.5 MB 93.0 MB/s eta 0:00:01\n",
      "     ---------------------------------       20.8/24.5 MB 93.0 MB/s eta 0:00:01\n",
      "     --------------------------------------  24.5/24.5 MB 93.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  24.5/24.5 MB 93.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 24.5/24.5 MB 50.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama_index) (3.0.3)\n",
      "Requirement already satisfied: colorama in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama_index) (3.20.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama_index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama_index) (2023.4)\n",
      "Requirement already satisfied: packaging>=17.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama_index) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.16.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\pycharmprojects\\adam\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.16.0)\n",
      "Installing collected packages: PyYAML, pypdf, PyMuPDFb, pillow, pymupdf, bs4, llama-index-legacy, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-embeddings-openai, llama-index-program-openai, llama-index-multi-modal-llms-openai, llama-index-agent-openai, llama-index-question-gen-openai, llama_index\n",
      "  Attempting uninstall: llama_index\n",
      "    Found existing installation: llama-index 0.9.45\n",
      "    Uninstalling llama-index-0.9.45:\n",
      "      Successfully uninstalled llama-index-0.9.45\n",
      "Successfully installed PyMuPDFb-1.23.22 PyYAML-6.0.1 bs4-0.0.2 llama-index-agent-openai-0.1.1 llama-index-core-0.10.3 llama-index-embeddings-openai-0.1.1 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.1 llama-index-multi-modal-llms-openai-0.1.1 llama-index-program-openai-0.1.1 llama-index-question-gen-openai-0.1.1 llama-index-readers-file-0.1.3 llama_index-0.10.4 pillow-10.2.0 pymupdf-1.23.22 pypdf-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: llama-index-integrations/llms/llama-index-llms-ollama is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with bzr+http, bzr+https, bzr+ssh, bzr+sftp, bzr+ftp, bzr+lp, bzr+file, git+http, git+https, git+ssh, git+git, git+file, hg+file, hg+http, hg+https, hg+ssh, hg+static-http, svn+ssh, svn+http, svn+https, svn+svn, svn+file).\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'download_loader' from 'llama_index.core' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from llama_index.core import SimpleDirectoryReader\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_loader\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatabase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatabaseReader\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'download_loader' from 'llama_index.core' (unknown location)"
     ]
    }
   ],
   "source": [
    "# from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import download_loader\n",
    "\n",
    "from llama_index.readers.database import DatabaseReader\n",
    "from llama_index.core import Document\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "\n",
    "class RAG():\n",
    "    def __init__(self, index_name):\n",
    "        self.index_name = index_name\n",
    "        self.pinecone_client = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "        self.pinecone_index = self.pinecone_client.Index(self.index_name)\n",
    "        self.vector_store = PineconeVectorStore(pinecone_index=self.pinecone_index)\n",
    "        self.index = VectorStoreIndex.from_vector_store(vector_store=self.vector_store)\n",
    "        # self.query_engine = self.index.as_query_engine()\n",
    "        # self.gemini_response = self.query_engine.query(\"\"\"What is the summary of the context providedd\n",
    "        #                              \"\"\")\n",
    "\n",
    "    def get_response(self):\n",
    "        return self.gemini_response\n",
    "    \n",
    "    # Data ingestion methods\n",
    "    # def ingest_directory(self, \n",
    "    #                      directory_path=\"./database/adam_RAG\"):\n",
    "    #     documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    #     return documents \n",
    "    \n",
    "    def ingest_database(self,\n",
    "                        query = \"SELECT * FROM hadiths\"):\n",
    "        \n",
    "            reader = DatabaseReader(\n",
    "                scheme=os.getenv(\"DB_SCHEME\"),\n",
    "                host=os.getenv(\"DB_HOST\"),\n",
    "                port=os.getenv(\"DB_PORT\"),\n",
    "                user=os.getenv(\"DB_USER\"),\n",
    "                password=os.getenv(\"DB_PASS\"),\n",
    "                dbname=os.getenv(\"DB_NAME\"),\n",
    "            )\n",
    "\n",
    "            query = \"SELECT * FROM users\"\n",
    "            documents = reader.load_data(query=query)\n",
    "            return documents\n",
    "    \n",
    "    def ingest_web(self, url):\n",
    "        BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "        loader = BeautifulSoupWebReader()\n",
    "        documents = loader.load_data(urls=[url])\n",
    "        return documents\n",
    "    \n",
    "    def ingest_text(self, text):\n",
    "        return Document(text=text)\n",
    "    \n",
    "\n",
    "    def high_level_transformation(self, \n",
    "                                  documents, \n",
    "                                  prompt=\"hey i feel like taking a trip that follows the exact footsteps of Moses,starting from egypt. so give me a itenerary for a 14 day trip. \"):\n",
    "        vector_index = VectorStoreIndex.from_documents(documents)\n",
    "        vector_index.as_query_engine()\n",
    "\n",
    "        response = query_engine.query(prompt)\n",
    "        print(response)\n",
    "        # text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "        # Settings.text_splitter = text_splitter\n",
    "\n",
    "        # # per-index\n",
    "        # index = VectorStoreIndex.from_documents(\n",
    "        #     documents, transformations=[text_splitter]\n",
    "        # )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a great hue and cry in the Royal Court.\n"
     ]
    }
   ],
   "source": [
    "# query pinecone index for similar embeddings\n",
    "query_engine = index.as_query_engine()\n",
    "gemini_response = query_engine.query(\"\"\"According to the context provided,\n",
    "                                    What happened after Heraclius read the letter?\n",
    "                                     \"\"\")\n",
    "\n",
    "# print response\n",
    "import textwrap\n",
    "print(textwrap.fill(str(gemini_response), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I cannot answer your query.\n"
     ]
    }
   ],
   "source": [
    "# how to view the index itself\n",
    "# i want to see how the embeddings look like\n",
    "chat_engine = index.as_chat_engine()\n",
    "gemini_response = chat_engine.query(\"\"\"\n",
    "                                    According to the context provided,\n",
    "                                    How did Heraclius feel about the Prophet?\n",
    "                                    \"\"\")\n",
    "\n",
    "# print response\n",
    "import textwrap\n",
    "print(textwrap.fill(str(gemini_response), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x148f83db850>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2152.57048092869"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1_298_000/603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766.2832056194126"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2_399_999/3132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
