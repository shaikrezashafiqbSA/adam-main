{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag V2 Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RAG.RAG import RAG\n",
    "rag = RAG(\n",
    "    data_models=[\n",
    "                #  {\"name\":\"travel\",\n",
    "                #   \"type\":\"db\",\n",
    "                #   \"data\": [\"flights\", \"accommodations\", \"activities\", \"services\", \"insurance\"],\n",
    "                #   \"LLM_model\":\"gemini-pro-ultra\",\n",
    "                #   \"embedding_model\":\"models/embedding-001\"},\n",
    "\n",
    "                #   {\"name\":\"islam\",\n",
    "                #    \"type\":\"db\",\n",
    "                #    \"data\": [\"hadith\", \"quran\"],\n",
    "                #    \"LLM_model\":\"claude-3-opus-20240229\",\n",
    "                #    \"embedding_model\":\"models/embedding-001\"},\n",
    "\n",
    "                   {\"name\": \"hadith.com\",\n",
    "                    \"type\": \"file\",\n",
    "                    \"LLM_model\": \"claude-3-opus-20240229\",\n",
    "                    \"embedding_model\": \"models/embedding-001\"},\n",
    "                ],\n",
    "        ) \n",
    "\n",
    "result = rag.query(\n",
    "    # query_text=\"plan me a travel package: i want replicate the experience of Moses travelling from Egypt to sinai, with a side quest with Al-khidr next month\",\n",
    "    query_text=\"Is there a hadith that mentions the story of Moses and Al-khidr?\",\n",
    "    # budget_constraint = 10000,\n",
    "    # data_sources=[\"travel\", \"islam\"],\n",
    "    # supplementary=[\"AI_video_of_Moses_and_Al-khidr.mp4\"],\n",
    "    # focus=\"historical\",  # Optional focus parameter: \"historical\", \"religious\", \"cultural\", \"adventure\", \"educational\"\n",
    "    instruction_mapper = {\n",
    "                            \"models\": [ \n",
    "                                {\"type\": \"NLP\", \"model\": \"spaCy\"},           # favor rule-based extraction for efficiency.\n",
    "                                # {\"type\": \"LLM\", \"model\": \"gemini-pro-ultra\"} # rely on the LLM to interpret subtleties.\n",
    "                            ] # Complex Query: The list format above would enable a sequential hybrid approach leveraging both techniques.\n",
    "                         } \n",
    ")\n",
    "\n",
    "# Example result (structured)\n",
    "# print(result[\"travel_package\"]) \n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Hadith found containing 'Al-Khidr'.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Hadiths found in Hisn al-Muslim section.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import api_key, Completion  # Assuming you're using the OpenAI API\n",
    "def construct_prompt(instruction, text_chunk):\n",
    "    return f\"Instruction: {instruction} \\nDocument Text: {text_chunk} \\nAnswer:\"\n",
    "\n",
    "def RagParse(parsing_instruction, result_type=\"markdown\",):\n",
    "    \"\"\"\n",
    "    RagParse function primarily leveraging an LLM for parsing instruction execution. \n",
    "    \"\"\"\n",
    "    # ... (Setup your API Key if needed) ...\n",
    "\n",
    "    def load_data(data_path):\n",
    "        try:\n",
    "            with open(data_path, \"r\") as f:\n",
    "                document_text = f.read() \n",
    "            return {\"document_text\": document_text}  \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found at {data_path}\")\n",
    "            return {}\n",
    "\n",
    "    def process_document(document):\n",
    "        benefits_strings = []\n",
    "\n",
    "        # (1) Split document into potential sections or chunks \n",
    "        document_chunks = split_document(document[\"document_text\"])  \n",
    "\n",
    "        for chunk in document_chunks:\n",
    "            # (2) LLM Prompt Construction for Each Chunk\n",
    "            prompt = construct_prompt(parsing_instruction, chunk) \n",
    "\n",
    "            # (3) Call Gemini API\n",
    "            response = Completion.create(\n",
    "                engine=\"gemini-pro-ultra-0225\",  # Or your preferred engine\n",
    "                prompt=prompt,\n",
    "                max_tokens=100,  # Adjust as needed\n",
    "                # ... other API parameters as needed ...\n",
    "            )\n",
    "\n",
    "            # (4) Extract and Process Response \n",
    "            extracted_strings = extract_benefits_strings(response.choices[0].text)\n",
    "            benefits_strings.extend(extracted_strings)\n",
    "\n",
    "        return format_output(benefits_strings, result_type)\n",
    "\n",
    "    return process_document(load_data(\"./policy.pdf\")) # Change the path as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP AI Platform \n",
    "Notebooks are pre-installed with the Rag V2 framework. This notebook demonstrates how to use the Rag V2 framework to fine-tune a Rag model on a custom dataset that is stored in a Google Cloud Storage bucket.\n",
    "- The dataset should be in the form of a jsonl file with the following format:\n",
    "```\n",
    "{\"text\": \"question\", \"meta\": {\"name\": \"document_name\", \"section\": \"document_section\"}}\n",
    "{\"text\": \"answer\", \"meta\": {\"name\": \"document_name\", \"section\": \"document_section\"}}\n",
    "```\n",
    "\n",
    "- The model is fine-tuned on the dataset using the `rag_v2` framework.\n",
    "- The fine-tuned model is saved to a Google Cloud Storage bucket.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "1. Load the dataset from a Google Cloud Storage bucket.\n",
    "2. Fine-tune the Rag model on the dataset.\n",
    "3. Save the fine-tuned model to a Google Cloud Storage bucket.\n",
    "4. Test the fine-tuned model.\n",
    "5. Inference using the fine-tuned model + database of documents.\n",
    "\n",
    "    a) Load the documents from a Google Cloud Storage bucket.\n",
    "\n",
    "    b) Index the documents using embedding-model (LLM/NLP)\n",
    "\n",
    "        i) Generate embeddings for the documents.\n",
    "\n",
    "        ii) Index the embeddings\n",
    "\n",
    "    c) Query the fine-tuned model with a question.\n",
    "\n",
    "        i) Retrieve the top-k documents.\n",
    "\n",
    "        ii) Retrieve the answer from the indexed documents.\n",
    "\n",
    "    d) Retrieve the answer from the indexed documents.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) GCP API\n",
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Master Database']\n"
     ]
    }
   ],
   "source": [
    "import gspread\n",
    "\n",
    "gc = gspread.service_account(filename='./gdrive/phrasal-ability-419201-d527372ace3b.json') \n",
    "# gc = gspread.service_account(filename='./gdrive/lunar-landing-389714-369d3f1b2a09.json')\n",
    "sheets = gc.openall()\n",
    "print([sheet.title for sheet in sheets]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.499999999999996\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# Bedrock Command\n",
    "cost_b = (1.5/1e6 + 2/1e6)*5000*1000\n",
    "print(cost_b)\n",
    "# cohere plus\n",
    "cost_c = (0.5/1e6 + 1.5/1e6)*5000*1000\n",
    "print(cost_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0001 *1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
